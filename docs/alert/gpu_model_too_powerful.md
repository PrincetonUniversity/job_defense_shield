# GPU Model Too Powerful

This alert is used to identify jobs that could have ran on less powerful GPUs.
For example, it can find jobs that ran on NVIDIA H100 GPUs but could have
used the less powerful L40S GPUs or [MIG](https://www.nvidia.com/en-us/technologies/multi-instance-gpu/).
The GPU utilization, CPU/GPU memory usage, and number of allocated CPU-cores
is taken into account when identifying jobs.

## Configuration File

Here is an example entry for `config.yaml`:

```yaml
gpu-model-too-powerful:
  clusters:
    - della
  partitions:
    - gpu
  min_run_time: 61        # minutes
  num_cores_threshold: 1  # count
  num_gpus_threshold: 1   # count
  gpu_util_threshold: 15  # percent
  gpu_mem_threshold: 10   # GB
  cpu_mem_threshold: 32   # GB
  email_file: "gpu_model_too_powerful.txt"
  admin_emails:
    - admin@institution.edu
```

- `min_run_time`: The minimum run time of a job for it to be considered. Jobs that did not run longer
than this limit will be ignored. Default: 0

- `num_cores_threshold`: Only jobs that allocate a number of CPU-cores that is equal to or less than `num_cores_threshold` will be included.

- `num_gpus_threshold`: The number of allocated GPUs to be considered by the alert.

- `gpu_util_threshold`:  The GPU utilization as available from `nvidia-smi`. Jobs with a mean GPU utilization of less or equal to this value will be included. The default value is 15%.

- `gpu_util_target`: The minimum acceptable mean GPU utilization for any job. This is available as a email tag (see below):

- `email_file`: The text file to be used for the email message.

- `email_subject`: Subject of the email message to users.

- `include_running_jobs`: (Optional) If `True` then jobs in a state of `RUNNING` will be included in the calculation. The Prometheus server must be queried for each running job, which can be an expensive operation. Default: False

- `nodelist`: (Optional) Only apply this alert to jobs that ran on the specified nodes. See [example](../nodelist.md).

- `excluded_users`: (Optional) List of users to exclude from receiving emails. These users will still appear
in reports for system administrators when `--report` is used.

- `admin_emails`: (Optional) The emails sent to users will also be sent to these administator emails. This applies when the `--email` option is used.

## Report

```bash
$ python job_defense_shield.py --gpu-model-too-powerful

          GPU Model Too Powerful         
-----------------------------------------
 User   GPU-Hours  Jobs   JobID    Emails
-----------------------------------------
u87203     1.1       1   61122477   2 (1)
```

## Email Message

Below is an example email message (see `email/gpu_model_too_powerful.txt`):

```
Hello Alan (u12345),

Below are jobs that ran on an A100 GPU on Della in the past 10 days:

   JobID    User  GPU-Util GPU-Mem-Used CPU-Mem-Used  Hours
  60984405 aturing   9%        2 GB         3 GB      3.4  
  60984542 aturing   8%        2 GB         3 GB      3.0  
  60989559 aturing   8%        2 GB         3 GB      2.8  

The jobs above have a low GPU utilization and they use less than 10 GB of GPU
memory and less than 32 GB of CPU memory. Such jobs could be run on the MIG
GPUs. A MIG GPU has 1/7th the performance and memory of an A100. To run on a
MIG GPU, add the "partition" directive to your Slurm script:

  #SBATCH --nodes=1
  #SBATCH --ntasks=1
  #SBATCH --cpus-per-task=1
  #SBATCH --gres=gpu:1
  #SBATCH --partition=mig

For interactive sessions use, for example:

  $ salloc --nodes=1 --ntasks=1 --time=1:00:00 --gres=gpu:1 --partition=mig

Replying to this automated email will open a support ticket with Research
Computing.
```

### Tags

The following tags can be used in the email file:

- `<GREETING>`: The greeting generated by `greeting-method`.
- `<CLUSTER>`: The cluster specified for the alert (i.e., `cluster`).
- `<PARTITIONS>`: The partitions listed for the alert (i.e., `partitions`).
- `<TARGET>`: The GPU utilization (i.e., `gpu_util_target`).
- `<DAYS>`: Number of days in the time window (default is 7).
- `<NUM-JOBS>`: Total number of jobs that fit the constraints.
- `<TABLE>`: Table of job data.
- `<JOBSTATS>`: `jobstats` command for the first JobID (`$ jobstats 12345678`).

## Usage

Send emails to users with jobs that could have used less powerful GPU models:

```
$ python job_defense_shield.py --gpu-model-too-powerful --email  --clusters=della --partition=gpu
```

## Tip

Be aware that a `nodelist` can be specified. This makes it possible to isolate jobs that ran on certain nodes. See the [nodelist](../nodelist.md) example.
