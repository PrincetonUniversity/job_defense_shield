{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Defend the Hardware","text":"<p>Job Defense Shield is a software tool for identifying and reducing instances of underutilization by the users of high-performance computing systems. The software can (1) send automated email alerts to users, (2) create reports for system administrators, and (3) automatically cancel GPU jobs at 0% utilization. Job Defense Shield is a component of the Jobstats job monitoring platform.</p> <p>Most popular feature:</p> <ul> <li>automatically cancel GPU jobs at 0% utilization</li> </ul> <p>Automated email alerts and reports are available for:</p> <ul> <li>low GPU utilization</li> <li>too many allocated CPU-cores per GPU</li> <li>too much allocated CPU memory per GPU</li> <li>GPU model was too powerful (i.e., use MIG instead)</li> <li>multinode GPU fragmentation</li> <li>excessive run time limits for GPU jobs</li> <li>over-allocating CPU memory</li> <li>low CPU utilization</li> <li>serial jobs allocating multiple CPU-cores</li> <li>multinode CPU fragmentation</li> <li>excessive run time limits for CPU jobs</li> </ul>"},{"location":"#example-reports","title":"Example Reports","text":"<p>Which users have wasted the most GPU-hours?</p> <pre><code>                         GPU-Hours at 0% Utilization                          \n---------------------------------------------------------------------\n    User   GPU-Hours-At-0%  Jobs             JobID             Emails\n---------------------------------------------------------------------\n1  u12998        308         39   62285369,62303767,62317153+   1 (3)\n2  u9l487         84         14   62301737,62301738,62301742+   0     \n3  u39635         25          2            62184669,62187323    2 (4)     \n4  u24074         24         13   62303182,62303183,62303184+   0      \n---------------------------------------------------------------------\n   Cluster: della\nPartitions: gpu, llm\n     Start: Wed Feb 12, 2025 at 09:50 AM\n       End: Wed Feb 19, 2025 at 09:50 AM\n</code></pre> <p>Which users are over-allocating the most CPU memory?</p> <pre><code>                        Users Allocating Excess CPU Memory                 \n----------------------------------------------------------------------------\n    User    Unused    Used    Ratio   Ratio  Ratio   CPU-Hrs  Jobs   Emails\n           (TB-Hrs) (TB-Hrs) Overall  Mean   Median                        \n----------------------------------------------------------------------------\n1  u93714    127       10      0.07   0.08   0.07     42976    12      2 (6)  \n2  u44210     17       81      0.83   0.84   0.79     31082    20      0  \n3  u61098     10        4      0.71   0.71   0.65      6790     4      0\n4  u13158      4        1      0.20   0.20   0.20      3961     2      0  \n----------------------------------------------------------------------------\n   Cluster: tiger\nPartitions: cpu\n     Start: Wed Feb 12, 2025 at 09:50 AM\n       End: Wed Feb 19, 2025 at 09:50 AM\n</code></pre>"},{"location":"#example-emails","title":"Example Emails","text":"<p>Below is an example email to a user for the automatic cancellation of GPU jobs:</p> <pre><code>Hi Alan (u12345),\n\nThe jobs below have been cancelled because they ran for 2 hours at 0% GPU\nutilization:\n\n     JobID    Cluster  Partition    State    GPUs-Allocated GPU-Util  Hours\n    60131148   della      gpu     CANCELLED         4          0%       2  \n    60131741   della      gpu     CANCELLED         4          0%       2  \n\nSee our GPU Computing webpage for three common reasons for encountering zero GPU\nutilization:\n\n    https://your-institution.edu/KB/gpu-computing\n\nReplying to this automated email will open a support ticket with Research\nComputing.\n</code></pre> <p>Below is an example email to a user that is allocating too much CPU memory:</p> <pre><code>Hi Alan (u12345),\n\nBelow are your jobs that ran on Stellar in the past 7 days:\n\n     JobID   Memory-Used  Memory-Allocated  Percent-Used  Cores  Hours\n    5761066      2 GB          100 GB            2%         1     48\n    5761091      4 GB          100 GB            4%         1     48\n    5761092      3 GB          100 GB            3%         1     48\n\nIt appears that you are requesting too much CPU memory for your jobs since\nyou are only using on average 3% of the allocated memory. For help on\nallocating CPU memory with Slurm, please see:\n\n    https://your-institution.edu/KB/cpu-memory\n\nReplying to this automated email will open a support ticket with Research\nComputing.\n</code></pre>"},{"location":"#example-usage","title":"Example Usage","text":"<p>Send emails to users that are over-allocating CPU memory:</p> <pre><code>$ job_defense_shield --excess-cpu-memory --email\n</code></pre> <p>If you think that Job Defense Shield is a fit for your institution then continue to the Installation page.</p>"},{"location":"configuration/","title":"Configuration","text":"<p>Job Defense Shield is configured using a YAML file, which specifies global settings as well as the individual alerts to run.</p> <p>Below is a minimal configuration file (<code>config.yaml</code>) with one alert:</p> <pre><code>%YAML 1.1\n---\n#####################\n## GLOBAL SETTINGS ##\n#####################\njobstats-module-path: /path/to/jobstats/module/\njobstats-config-path: /path/to/jobstats/config/\nviolation-logs-path:  /path/to/violations/\nemail-files-path:     /path/to/email/\nemail-domain-name: \"@institution.edu\"\nsender:   support@institution.edu\nreply-to: support@institution.edu\ngreeting-method: getent\nworkday-method: file\nholidays-file: /path/to/holidays.txt\nreport-emails:\n  - admin1@institution.edu\n  - admin2@institution.edu\nverbose: False\nshow-empty-reports: False\n\n\n##################################\n## ZERO CPU UTILIZATION (ALERT) ##\n##################################\nzero-cpu-utilization-1:\n  cluster: stellar\n  partitions:\n    - cpu\n    - bigmem\n  min_run_time: 61 # minutes\n  email_file: \"zero_cpu_utilization.txt\"\n  admin_emails:\n    - job-alerts-aaaalegbihhpknikkw2fkdx6gi@institution.slack.com\n    - admin@institution.edu\n</code></pre> <p>Each line of the global settings is explained below. See example.yaml in the GitHub repository for a full configuration file example.</p>"},{"location":"configuration/#global-settings","title":"Global Settings","text":""},{"location":"configuration/#jobstats-module-and-config-paths","title":"Jobstats Module and Config Paths","text":"<p>These paths are only needed for addressing the underutilization of actively running jobs:</p> <pre><code>jobstats-module-path: /path/to/jobstats/module/  # jobstats.py\njobstats-config-path: /path/to/jobstats/config/  # config.py\n</code></pre> <p>If you are only interested in completed jobs then you do not need this feature. In this case, remove these lines from <code>config.yaml</code>.</p> <p>Here is an example for a particular system:</p> <pre><code>jobstats-module-path: /usr/local/jobstats/\njobstats-config-path: /etc/jobstats/\n</code></pre> <p>The value of <code>PROM_SERVER</code> is taken from <code>config.py</code>.</p>"},{"location":"configuration/#violation-logs","title":"Violation Logs","text":"<p>One must specify the path to a writable directory to store the underutilization history of each user:</p> <pre><code>violation-logs-path: /path/to/violations/\n</code></pre> <p>If the path does not exist then the software will try to make it.</p> <p>The files stored in this directory are read when deciding whether or not sufficient time has passed to send the user another email. These files are important and we recommend maintaining a backup.</p>"},{"location":"configuration/#email-settings","title":"Email Settings","text":"<p>Set the path to your email files. A set of example files is found in the <code>email</code> directory of the Job Defense Shield GitHub repository. It is recommended to copy the example email files to another directory outside of <code>job_defense_shield</code> and put that under version control. As you will learn, placeholders like <code>&lt;GREETING&gt;</code> are replaced by the actual values as computed in the Python code.</p> <pre><code>email-files-path: /path/to/email/\n</code></pre> <p>Specify the email domain for your institution:</p> <pre><code>email-domain-name: \"@institution.edu\"\n</code></pre> <p>Usernames will be concatenated with the email domain to make user email addresses.</p> <p>Specify the <code>sender</code> and <code>reply-to</code> values for sending emails:</p> <pre><code>sender: support@institution.edu\nreply-to: support@institution.edu\n</code></pre> <p>Tip</p> <p>By using a <code>reply-to</code> that is different from <code>sender</code>, one can prevent auto-reply or out-of-office emails from creating new support tickets. If this choice is made then it is likely that users will need to forward any underutilization emails they receive to <code>sender</code> to open a new support ticket. Keep this issue in mind when writing the email messages.</p> <p>Use the <code>greeting-method</code> to determine the first line of the email that users receive:</p> <pre><code>greeting-method: getent\n</code></pre> <p>The <code>getent</code> method will call <code>getent passwd</code> on the username to find the first name of the user in producing a greeting such as:</p> <pre><code>Hello Alan (u12345),\n</code></pre> <p>A choice of <code>basic</code> will produce:</p> <pre><code>Hello u12345,\n</code></pre> <p>There is also <code>ldap</code> which calls <code>ldapsearch</code>. Our recommendation is <code>getent</code>. If you find that <code>getent</code> is not working properly during testing then use <code>basic</code>.</p> <p>Lastly, one can create reports and have those sent to administrators by email when the <code>--report</code> flag is used:</p> <pre><code>report-emails:\n  - admin1@institution.edu\n  - admin2@institution.edu\n</code></pre>"},{"location":"configuration/#optional-external-smtp-server-for-sending-emails","title":"(Optional) External SMTP Server for Sending Emails","text":"<p>By default, Job Defense Shield will use a local SMTP server using <code>localhost</code> to send emails. This will work for almost all institutions and no configuration is needed.</p> <p>If the local server is insufficient then one can specify an external SMTP server with TLS encryption by adding the following settings to the configuration file:</p> <pre><code>smtp-server: smtp.example.edu\nsmtp-user: username\nsmtp-port: 587\n</code></pre> <p>The password is set using the following environment variable:</p> <pre><code>export JOBSTATS_SMTP_PASSWORD=********\n</code></pre> <p>Alternatively, one can specify the password in the configuration file:</p> <pre><code>smtp-server: smtp.example.edu\nsmtp-user: username\nsmtp-password: ********\nsmtp-port: 587\n</code></pre>"},{"location":"configuration/#workdays","title":"Workdays","text":"<p>Email alerts are only sent to users on workdays. Pick a method to distinguish the workdays from weekends and holidays. The most flexible method is <code>file</code>:</p> <pre><code>workday-method: file\nholidays-file: /path/to/holidays.txt\n</code></pre> <p>The file <code>holidays.txt</code> should be a list of dates with the format YYYY-MM-DD:</p> <pre><code>$ cat holidays.txt\n2025-05-26\n2025-06-19\n2025-07-04\n</code></pre> <p>If you only want to avoid weekends and U.S. Federal holidays then use:</p> <pre><code>workday-method: usa\n</code></pre> <p>If every day is a workday then:</p> <pre><code>workday-method: always\n</code></pre> <p>The <code>cron</code> setting can be used to avoid weekends so really this section is about dealing with holidays.</p>"},{"location":"configuration/#other-settings","title":"Other Settings","text":"<p>Partition names can be renamed:</p> <pre><code>partition-renamings:\n  datascience: datasci\n</code></pre> <p>If a partition is renamed then the new name must be used throughout the configuration file.</p> <p>For users that do not use their institutional email address, one can specify external addresses:</p> <pre><code>external-emails:\n  u12345: alan.turing@gmail.com\n  u23456: einstein@yahoo.com\n</code></pre> <p>Additional information is available by turning on the <code>verbose</code> setting. This will show the individual alerts and the jobs that are being ignored (e.g., due to missing metrics). The recommendation is to keep this turned off:</p> <pre><code>verbose: False\n</code></pre> <p>One can show or hide empty reports:</p> <pre><code>show-empty-reports: True\n</code></pre>"},{"location":"configuration/#specifying-a-custom-configuration-file","title":"Specifying a Custom Configuration File","text":"<p>By default the software will look for <code>config.yaml</code> in the same directory as <code>job_defense_shield.py</code> and then in the current working directory. One can explicitly specify the full path using the <code>--config-file</code> option:</p> <pre><code>$ job_defense_shield --config-file=/path/to/myconfig.yaml --low-gpu-efficiency\n</code></pre> <p>The ability to use different configuration files provides additional flexibility. For instance, for some institutions it may make sense to have a different configuration file for each cluster or for different alerts.</p>"},{"location":"configuration/#each-alert-must-have-a-different-name","title":"Each Alert Must Have a Different Name","text":"<p>Consider the following two alerts (pay attention to the alert names):</p> <pre><code>zero-cpu-utilization-1:\n  cluster: stellar\n  partitions:\n    - cpu\n  email_file: \"zero_cpu_utilization.txt\"\n  admin_emails:\n    - admin@institution.edu\n\nzero-cpu-utilization-1:\n  cluster: della\n  partitions:\n    - physics\n  email_file: \"zero_cpu_utilization.txt\"\n  admin_emails:\n    - admin@institution.edu\n</code></pre> <p>While the two alerts above are written for different clusters, only the second one will run since both alerts have the same name (<code>zero-cpu-utilization-1</code>).</p> <p>Warning</p> <p>Make sure each alert name has a different number at the end. An alert with the same name as one previously defined will override the previous alert.</p> <p>The corrected version would be:</p> <pre><code>zero-cpu-utilization-1:\n  cluster: stellar\n  partitions:\n    - cpu\n  email_file: \"zero_cpu_utilization.txt\"\n  admin_emails:\n    - admin@institution.edu\n\nzero-cpu-utilization-2:\n  cluster: della      \n  partitions:\n    - physics\n  email_file: \"zero_cpu_utilization.txt\"\n  admin_emails:\n    - admin@institution.edu\n</code></pre> <p>The second alert now has the name <code>zero-cpu-utilization-2</code>.</p>"},{"location":"configuration/#include-or-fully-remove-a-setting","title":"Include or Fully Remove a Setting","text":"<p>There are many optional settings for each alert. If you do not want to use an optional setting then fully remove the line.</p> <p>The following is incorrect for <code>min_run_time</code>:</p> <pre><code>zero-cpu-utilization-1:\n  cluster: stellar\n  partitions:\n    - cpu\n  min_run_time:\n  email_file: \"zero_cpu_utilization.txt\"\n  admin_emails:\n    - admin@institution.edu\n</code></pre> <p>If the default value for <code>min_run_time</code> should be used then completely remove the line. Here is the corrected entry:</p> <pre><code>zero-cpu-utilization-1:\n  cluster: stellar\n  partitions:\n    - cpu\n  email_file: \"zero_cpu_utilization.txt\"\n  admin_emails:\n    - admin@institution.edu\n</code></pre> <p>Another correct way is to specify the value:</p> <pre><code>zero-cpu-utilization-1:\n  cluster: stellar\n  partitions:\n    - cpu\n  min_run_time: 0  # minutes\n  email_file: \"zero_cpu_utilization.txt\"\n  admin_emails:\n    - admin@institution.edu\n</code></pre>"},{"location":"configuration/#full-example-configuration-file","title":"Full Example Configuration File","text":"<p>For more examples see example.yaml in the GitHub repository.</p>"},{"location":"configuration/#writing-and-testing-custom-emails","title":"Writing and Testing Custom Emails","text":"<p>See the next section to learn about sending custom emails to your users.</p>"},{"location":"contributions/","title":"Contributions","text":"<p>Contributions to the Jobstats platform and its tools are welcome. To work with the code, build a Conda environment:</p> <pre><code>$ conda create --name jds-env python=3.12     \\\n                              pandas          \\\n                              pyarrow         \\\n                              pytest-mock     \\\n                              ruff            \\\n                              blessed         \\\n                              requests        \\\n                              pyyaml          \\\n                              mkdocs-material \\\n                              -c conda-forge -y\n</code></pre>"},{"location":"contributions/#testing","title":"Testing","text":"<p>Be sure that the tests are passing before making a pull request:</p> <pre><code>(jds-env) $ pytest\n</code></pre> <p>There are additional options for development:</p> <pre><code>(jds-env) $ pytest  --cov=. --capture=tee-sys tests\n(jds-env) $ pytest -s tests  # use the -s option to run print statements\n</code></pre>"},{"location":"contributions/#static-checking","title":"Static Checking","text":"<p>Run <code>ruff</code> and make sure it is passing for each source file modified:</p> <pre><code>(jds-env) $ ruff check myfile.py\n</code></pre>"},{"location":"contributions/#documentation","title":"Documentation","text":"<p>The documentation is generated with Material for MkDocs. To build and serve the documentation:</p> <pre><code>(jds-env) $ mkdocs build\n(jds-env) $ mkdocs serve\n# open a web browser\n</code></pre>"},{"location":"emails/","title":"Emails","text":""},{"location":"emails/#sending-custom-emails-to-users","title":"Sending Custom Emails to Users","text":"<p>To send emails to users, each alert entry requires a text file for <code>email_file</code>:</p> <pre><code>##################################\n## ZERO CPU UTILIZATION (ALERT) ##\n##################################\nzero-cpu-utilization-1:\n  cluster: stellar\n  partitions:\n    - cpu\n  email_file: \"zero_cpu_utilization.txt\"\n  admin_emails:\n    - admin@institution.edu\n</code></pre> <p>The location of the <code>email_file</code> is set in <code>config.yaml</code> by:</p> <pre><code>email-files-path: /path/to/email/\n</code></pre> <p>Here is an example <code>email_file</code>:</p> <pre><code>$ cat /path/to/email/zero_cpu_utilization.txt\n&lt;GREETING&gt;\n\nBelow are your recent jobs that did not use all of the allocated nodes:\n\n&lt;TABLE&gt;\n\nThe CPU utilization was found to be 0% on each of the unused nodes. You can see\nthis by running the \"jobstats\" command, for example:\n\n&lt;JOBSTATS&gt;\n\nPlease investigate the reason(s) that the code is not using all of the allocated\nnodes before running additional jobs.\n\nReplying to this automated email will open a support ticket with Research\nComputing.\n</code></pre> <p>There are three placeholders in the text file above: <code>&lt;GREETING&gt;</code>, <code>&lt;TABLE&gt;</code> and <code>&lt;JOBSTATS&gt;</code>. Each placeholder will be replaced by the actual value when creating the email. The resulting email will appear as:</p> <pre><code>Hello Alan (u12345),\n\nBelow are your recent jobs that did not use all of the allocated nodes:\n\n      JobID    Cluster  Nodes  Nodes-Unused CPU-Util-Unused  Cores  Hours\n    62734245    della     4          3             0%          12    2.3 \n    62734246    della     6          5             0%          12    2.4 \n\nThe CPU utilization was found to be 0% on each of the unused nodes. You can see\nthis by running the \"jobstats\" command, for example:\n\n    $ jobstats 62734245\n\nPlease investigate the reason(s) that the code is not using all of the allocated\nnodes before running additional jobs.\n\nReplying to this automated email will open a support ticket with Research\nComputing.\n</code></pre> <p>Placeholders can be placed anywhere in your <code>email_file</code>. For example, one can include a placeholder in the middle of a sentence:</p> <pre><code>Below are your jobs on &lt;CLUSTER&gt; that did not use all of the allocated nodes:\n</code></pre> <p>Each alert has a finite set of placeholders that may be used to generate custom emails. There are a set of example email files in the <code>email</code> directory of the GitHub repository. It is recommended that you copy these and modify them as you see fit. It might also be a good idea to put them under version control along with <code>config.yaml</code> and <code>holidays.txt</code>. It is also recommended to occasionally save the contents of your <code>crontab</code>.</p>"},{"location":"emails/#testing-the-sending-of-emails-to-users","title":"Testing the Sending of Emails to Users","text":"<p>If <code>config.yaml</code> has an entry for <code>low-gpu-efficiency</code> then an administrator can see the output by running the alert:</p> <pre><code>$ job_defense_shield --low-gpu-efficiency\n</code></pre> <p>One adds the <code>--email</code> flag to send emails to the offending users:</p> <pre><code>$ job_defense_shield --low-gpu-efficiency --email\n</code></pre> <p>For testing, one can add a second flag that will only send the emails to <code>admin_emails</code> and not the users:</p> <pre><code>$ job_defense_shield --low-gpu-efficiency --email --no-emails-to-users\n</code></pre> <p>The <code>--no-emails-to-users</code> flag will also prevent violation log files from being updated. This allows administrators to test and modify the email messages as well as tune the threshold values in <code>config.yaml</code>.</p> <p>There is one alert that requires one extra step for testing, which is Cancel 0% GPU Jobs. In this case, one should add the following settings to the alert entry:</p> <pre><code>  do_not_cancel: True\n  warnings_to_admin: True\n</code></pre>"},{"location":"emails/#when-are-emails-sent","title":"When Are Emails Sent?","text":"<p>Emails to users are most effective when sent sparingly. For this reason, there is a command-line parameter <code>--days</code> to specify the amount of time that must pass before the user can receive another email for the same instance of underutilization (e.g., low GPU efficiency). By default, this time period is 7 days.</p> <p>The emails sent to users either contain the individual jobs or a summary for that week. Note that users can receive multiple emails about the same type of underutilization if there are multiple alerts covering different partitions or different clusters.</p> <p>The <code>Email</code> column in the table below shows the number of emails that each user has received about this particular instance of underutilization:</p> <pre><code>$ job_defense_shield --zero-util-gpu-hours\n\n                         GPU-Hours at 0% Utilization\n---------------------------------------------------------------------\n    User   GPU-Hours-At-0%  Jobs             JobID             Emails\n---------------------------------------------------------------------\n1  u12998        308         39   62285369,62303767,62317153+   1 (3)\n2  u9l487         84         14   62301737,62301738,62301742+   0\n3  u39635         25          2            62184669,62187323    2 (4)\n4  u24074         24         13   62303182,62303183,62303184+   0\n---------------------------------------------------------------------\n   Cluster: della\nPartitions: gpu, llm\n     Start: Wed Feb 12, 2025 at 09:50 AM\n       End: Wed Feb 19, 2025 at 09:50 AM\n</code></pre> <p>The number in parentheses is the number of days since the last email was sent. For example, <code>2 (4)</code> means that the user has received 2 emails with the last one being sent 4 days ago. To see when the emails were sent, see check mode.</p>"},{"location":"emails/#hyperlinks","title":"Hyperlinks","text":"<p>Instead of explicitly displaying URLs in email messages, one can create hyperlinks. For example:</p> <pre><code>If you believe that the code is capable of using more than 1 CPU-core then\nconsider attending an in-person &lt;a href=\"https://your-institution.edu/support/help-sessions\"&gt;Research Computing help session&lt;/a&gt; for assistance.\n</code></pre> <p>Hyperlinks condense the email message which presumably makes it more effective.</p>"},{"location":"extra/","title":"Check Mode","text":"<p>Use the <code>--check</code> flag to see which users have recevied emails for a given alert and when they received them. Here is an example for low CPU efficiencies:</p> <pre><code>$ job_defense_shield --low-cpu-efficiency --check\n\n=====================================================================\n                   LOW CPU EFFICIENCY (EMAILS SENT)                  \n=====================================================================\n                                                                  today\n                                                                    |\n                                                                    V\n  u31915  _____  _____  _____  X____  _____  X____  X____  _____  __X\n  u92476  _____  _X___  _____  _____  _____  _____  _____  _____  ___\n  u39327  _X___  _X___  _____  ____X  _____  X____  _____  _____  ___\n  u96725  _____  _____  _____  _____  _____  ____X  _____  _____  ___\n  u72912  _____  X____  _____  _____  _____  _____  _____  _____  ___\n  u34783  _____  _____  _____  _____  _____  _X___  _____  _____  ___\n  u12458  _____  X____  _____  _____  _____  _____  _____  _____  ___\n  u52797  _____  _____  _____  _____  _____  _X___  __X__  _____  ___\n  u31979  _____  _____  ____X  ____X  _____  _____  _____  _____  ___\n  u64118  _____  _____  X____  _____  _____  _____  _____  _____  ___\n  u77201  _X___  _____  _____  _____  _____  _____  _____  _____  ___\n  u65254  _X___  _____  _____  _____  _____  _____  _____  _____  ___\n  u42983  __X__  __X__  __X__  _____  _____  _____  _____  _____  __X\n  u36357  X____  X____  X____  _____  _____  __X__  __X__  _____  ___\n  u13242  _____  _____  _____  _____  _____  _X___  _____  _____  ___\n  u84230  X____  X____  _____  _____  _____  _____  _____  _____  ___\n  u62968  _____  _____  _____  _____  _____  _____  _____  _____  __X\n  u48309  _____  _____  _____  _____  _____  X____  _____  _____  ___\n  u28569  X____  _____  _____  _____  _____  _____  _____  _____  ___\n  u56129  _____  _____  _____  _____  _____  X____  _____  _____  ___\n\n=====================================================================\nNumber of X: 36\nNumber of users: 20\nViolation files: /path/to/violations/low_cpu_efficiency/\n</code></pre> <p>Time increases from left to right with today being on the far right. Each character in each row corresponds to a day. Weekend days are blank. An <code>X</code> character means that a user was sent an email.</p> <p>The time window can be adjusted using the <code>--days</code> option:</p> <pre><code>$ job_defense_shield --low-cpu-efficiency --check --days=100\n</code></pre>"},{"location":"nodelist/","title":"Nodelist","text":"<p>For systems composed of nodes with different specifications, filtering jobs by <code>cluster</code> and <code>partitions</code> can be insufficient. To provide more control, a <code>nodelist</code> can be specified:</p> <pre><code>too-much-cpu-mem-per-gpu-1:\n  cluster: della\n  partitions:\n    - gpu\n  cores_per_node:          48  # count\n  gpus_per_node:            4  # count\n  cpu_mem_per_node:      1000  # GB\n  cpu_mem_per_gpu_target: 240  # GB\n  cpu_mem_per_gpu_limit:  250  # GB\n  email_file: \"too_much_cpu_mem_per_gpu_2.txt\"\n  nodelist:\n    - della-l01g1\n    - della-l01g2\n    - della-l01g3\n    - della-l01g4\n    - della-l01g5\n    - della-l01g6\n    - della-l01g7\n    - della-l01g8\n</code></pre> <p>The alert above will only consider jobs that ran exclusively on one or more nodes in the <code>nodelist</code>. This makes it possible to write alerts for partitions composed of heterogeneous hardware.</p>"},{"location":"publications/","title":"Publications and Presentations","text":"<p>Combating Underutilization with the Jobstats Job Monitoring Platform PEARC 2025, Columbus, OH (July 23, 2025) Poster: PDF </p> <p>Integration of Open OnDemand with the Jobstats Job Monitoring Platform Global Open OnDemand Conference, Harvard University, Cambridge, MA (March 19, 2025) Slides: PDF </p> <p>Jobstats: A Slurm-Compatible Job Monitoring Platform for CPU and GPU Clusters PEARC 2024, Providence, RI (July 24, 2024) Poster: PDF </p> <p>Jobstats: A Slurm-Compatible Job Monitoring Platform for CPU and GPU Clusters PEARC 2023, Portland, OR (July 25, 2023) Slides: PDF </p> <p>Jobstats: A Slurm-Compatible Job Monitoring Platform for CPU and GPU Clusters PEARC '23: Practice and Experience in Advanced Research Computing 2023: Computing for the Common Good, Association for Computing Machinery, Pages 102-108, Year 2023 Paper: PDF </p>"},{"location":"publications/#awards","title":"Awards","text":"<p>Job Defense Shield won \"Best Poster Award\" at PEARC 2025.</p>"},{"location":"setup/","title":"Installation","text":"<p>We assume that the Jobstats platform is available and working.</p> <p>Cancelling Jobs at 0% GPU Utilization</p> <p>To automatically cancel actively running jobs, the software must be ran as a user with sufficient privileges to call <code>scancel</code>. This may inform your decision of where to install the software and by which user. All of the other alerts and reports can be ran as a regular user.</p> <p>Python 3.7 is the minimum supported version. The required dependencies for Job Defense Shield are <code>pandas</code>, <code>pyarrow</code>, <code>pyyaml</code> and <code>requests</code>. The <code>requests</code> module is needed to address the underutilization of actively running jobs. In this case, the Prometheus server must be queried.</p> <p>Job Defense Shield can be installed from the Python Package Index (PyPI):</p> <pre><code>$ pip install job-defense-shield\n</code></pre> <p>To install the software into an isolated environment:</p> <pre><code>$ python -m venv jds-env\n$ source jds-env/bin/activate\n(jds-env) $ pip install --upgrade pip\n(jds-env) $ pip install job-defense-shield\n</code></pre> <p>At a later time, to upgrade to the latest version:</p> <pre><code>(jds-env) $ pip install --upgrade job-defense-shield\n</code></pre> <p>It is strongly recommended to always use the latest version.</p>"},{"location":"setup/#testing-the-installation","title":"Testing the Installation","text":"<p>The simplest test is to run the help menu:</p> <pre><code>$ job_defense_shield --help\n</code></pre> <p>If the command above failed then see Troubleshooting the Installation.</p> <p>Next, try running a simple informational alert. To do this, make a trivial configuration file called <code>config.yaml</code> in the current working directory with the following contents:</p> <p><pre><code>$ cat config.yaml\n</code></pre> <pre><code>%YAML 1.1\n---\n#####################\n## GLOBAL SETTINGS ##\n#####################\nviolation-logs-path: /path/to/writable/directory/\nemail-files-path: /path/to/readable/directory/\nemail-domain-name: \"@institution.edu\"\nsender: support@institution.edu\nreply-to: support@institution.edu\nreport-emails:\n  - admin@institution.edu\n</code></pre></p> <p>Tip</p> <p>If the path that you specify for <code>violation-logs-path</code> does not exist then the software will try to make it. You can use any valid path for now. In the next section, you will choose the production paths for <code>violation-logs-path</code> and <code>email-files-path</code>.</p> <p>Be sure to replace <code>email-domain-name</code>, <code>sender</code>, <code>reply-to</code> and <code>report-emails</code> with your values.</p> <p>To test the software, run this command (which does not send any emails):</p> <pre><code>$ job_defense_shield --usage-overview\n</code></pre> <p>The command above will show an overview of the number of CPU-hours and GPU-hours across all clusters and partitions in the Slurm database over the past 7 days. Here is an example:</p> <pre><code>$ job_defense_shield --usage-overview\n\nJob Defense Shield (1.x.y)\ngithub.com/PrincetonUniversity/job_defense_shield\n\nINFO: Fri May 9, 2025 at 5:00 PM\nINFO: Python 3.11.11 (main, Dec 11 2024, 16:28:39) [GCC 11.2.0]\nINFO: Pandas 2.2.3\nINFO: Configuration file is /home/aturing/software/jds/config.yaml\nINFO: Calling sacct ... done (15 seconds).\nINFO: Cleaning sacct data\n        268639 jobs in the raw dataframe\n        258427 jobs in the cleaned dataframe\n\n\n           Usage Overview          \n-----------------------------------------\nCluster   Users   CPU-Hours    GPU-Hours\n-----------------------------------------\n   della  464   1285938 (16%) 91714 (65%)\n stellar  149   6745324 (82%)  1926  (1%)\ntraverse    1    189987  (2%) 47497 (34%)\n-----------------------------------------\n\n\n\n          Usage Overview by Partition           \n------------------------------------------------------\nCluster  Partition  Users  CPU-Hours      GPU-Hours\n------------------------------------------------------\n   della        cpu  311    874114  (68%)     0   (0%)\n   della      pli-c   28    115406   (9%) 25838  (28%)\n   della gpu-shared   98     83617   (7%) 30083  (33%)\n   della    datasci   31     80954   (6%)     0   (0%)\n   della        gpu   51     47475   (4%) 16503  (18%)\n   della        pli   20     35814   (3%)  6249   (7%)\n   della     cryoem   17     20897   (2%)  4110   (4%)\n   della    physics    5     12968   (1%)     0   (0%)\n   della        mig   41      7169   (1%)  7169   (8%)\n   della     pli-lc    5      3107   (0%)  1081   (1%)\n   della    gputest   99      1948   (0%)   647   (1%)\n   della        all    1      1280   (0%)     0   (0%)\n   della      monia    4      1003   (0%)     0   (0%)\n   della     gpu-ee    2       173   (0%)    23   (0%)\n   della      grace    1        11   (0%)    11   (0%)\n   della      salik    1         2   (0%)     0   (0%)\n stellar      cimes   21   2941001  (44%)     0   (0%)\n stellar         pu   56   2426873  (36%)     0   (0%)\n stellar       pppl   33   1340776  (20%)     0   (0%)\n stellar     serial   41     13187   (0%)     0   (0%)\n stellar        all   48     12377   (0%)     0   (0%)\n stellar        gpu   20     11044   (0%)  1926 (100%)\n stellar     bigmem    1        66   (0%)     0   (0%)\ntraverse        all    1    189987 (100%) 47497 (100%)\n------------------------------------------------------\n     Start: Fri May 02, 2025 at 05:00 PM\n       End: Fri May 09, 2025 at 05:00 PM\n</code></pre> <p>You can go further back in time by using the <code>--days</code> option:</p> <pre><code>$ job_defense_shield --usage-overview --days=14\n</code></pre> <p>Info</p> <p>Using a large value for the <code>--days</code> option can cause the Slurm database to fail to produce the data. The default is 7 days.</p> <p>One can only include data from specific clusters or partitions using the <code>-M</code> and <code>-r</code> options:</p> <p><pre><code>$ job_defense_shield --usage-overview -M della -r cpu,gpu\n</code></pre> Or equivalently:</p> <pre><code>$ job_defense_shield --usage-overview --clusters=della --partition=cpu,gpu\n</code></pre> <p>The <code>-M</code> and <code>-r</code> options (or <code>--clusters</code> and <code>--partition</code>) can be used to reduce the load on the database server when an alert only applies to a particular cluster or particular partitions. These options are passed through to the Slurm command <code>sacct</code>. See <code>man sacct</code> for more information.</p>"},{"location":"setup/#email-test","title":"Email Test","text":"<p>By having your email address in <code>report-emails</code> in <code>config.yaml</code>, the <code>--report</code> flag can be used to send the output to administrators by email:</p> <pre><code>$ job_defense_shield --usage-overview --report\n</code></pre> <p>This feature is useful when combined with <code>cron</code>. Specifically, one can receive a daily report showing all of the instances of underutilization across all of the systems. More will be said about this later.</p> <p>If the email test failed then see the Configuration section for setting up an external SMTP server.</p>"},{"location":"setup/#troubleshooting-the-installation","title":"Troubleshooting the Installation","text":"<p>Make sure you are using the <code>python</code> in the isolated environment. All three commands below should run successfully:</p> <pre><code>$ python --version\n$ python -c \"import pandas; print(pandas.__version__)\"\n$ python -c \"import pyyaml; print(pyyaml.__version__)\"\n</code></pre> <p>If the configuration file is not found then try specifying the full path:</p> <pre><code>$ job_defense_shield --config-file=/path/to/config.yaml --usage-overview --report\n</code></pre>"},{"location":"setup/#creating-a-configuration-file-for-production","title":"Creating a Configuration File for Production","text":"<p>See the next section to learn how to write a proper configuration file.</p>"},{"location":"support/","title":"Support","text":"<p>For assistance with Job Defense Shield, please post an issue on the GitHub repository: https://github.com/PrincetonUniversity/job_defense_shield</p> <p>Please include the versions of Python and pandas that you are using.</p> <p>For assistance with the Jobstats platform, please post an issue on the Jobstats GitHub repository: https://github.com/PrincetonUniversity/jobstats</p>"},{"location":"alert/cancel_gpu_jobs/","title":"Automatically Cancel GPU Jobs at 0% Utilization","text":"<p>This alert cancels GPU jobs at 0% utilization.</p> <p>Elevated Privileges</p> <p>This alert is different than the others in that it must be ran as a user with sufficient privileges to call <code>scancel</code>.</p> <p>The software can be configured to cancel jobs based on GPU utilization during the first N minutes of a job (see <code>cancel_minutes</code>) and/or during the last N minutes (see <code>sliding_cancel_minutes</code>). Warning emails can be sent to the users before cancellation.</p> <p>The ability to automatically cancel GPU jobs is one of the most popular features of Jobstats.</p>"},{"location":"alert/cancel_gpu_jobs/#configuration-file","title":"Configuration File","text":"<p>Below is an example entry for the configuration file:</p> <pre><code>cancel-zero-gpu-jobs-1:\n  cluster: della\n  partitions:\n    - gpu\n    - llm\n  sampling_period_minutes: 15  # minutes\n  first_warning_minutes:   60  # minutes\n  second_warning_minutes: 105  # minutes\n  cancel_minutes:         120  # minutes\n  email_file_first_warning:  \"cancel_gpu_jobs_warning_1.txt\"\n  email_file_second_warning: \"cancel_gpu_jobs_warning_2.txt\"\n  email_file_cancel:         \"cancel_gpu_jobs_scancel_3.txt\"\n  sliding_warning_minutes: 240  # minutes\n  sliding_cancel_minutes:  300  # minutes\n  email_file_sliding_warning: \"cancel_gpu_jobs_sliding_warning.txt\"\n  email_file_sliding_cancel:  \"cancel_gpu_jobs_sliding_cancel.txt\"\n  jobid_cache_path: /path/to/writable/directory/\n  admin_emails:\n    - admin@institution.edu\n  excluded_users:\n    - u12345\n    - u23456\n</code></pre> <p>The settings are explained below:</p> <ul> <li> <p><code>cluster</code>: Specify the cluster name as it appears in the Slurm database.</p> </li> <li> <p><code>partitions</code>: Specify one or more Slurm partitions. Use <code>\"*\"</code> to include all partitions (i.e., <code>partitions: [\"*\"]</code>).</p> </li> <li> <p><code>sampling_period_minutes</code>: Number of minutes between executions of this alert. This number must be equal to the time between <code>cron</code> jobs for this alert (see cron section below). One reasonable choice for this setting is 15 minutes.</p> </li> <li> <p><code>first_warning_minutes</code>: (Optional) Send a warning email for 0% GPU utilization after this number of minutes from the start of the job.</p> </li> <li> <p><code>second_warning_minutes</code>: (Optional) Send a second warning email for 0% GPU utilization after this number of minutes from the start of the job.</p> </li> <li> <p><code>cancel_minutes</code>: (Optional/Required) Cancel jobs with 0% GPU utilization after this number of minutes from the start of the job. One must set <code>cancel_minutes</code> and/or <code>sliding_cancel_minutes</code>.</p> </li> <li> <p><code>email_file_first_warning</code>: (Optional) File to be used for the first warning email when <code>cancel_minutes</code> is set.</p> </li> <li> <p><code>email_file_second_warning</code>: (Optional) File to be used for the second warning email when <code>cancel_minutes</code> is set.</p> </li> <li> <p><code>email_file_cancel</code>: (Optional/Required) File to be used for the cancellation email. If <code>cancel_minutes</code> is set then this file is required.</p> </li> <li> <p><code>sliding_warning_minutes</code>: (Optional/Required) Send a warning email for jobs found with 0% GPU utilization during a sliding time window of this number of minutes. This setting is required if <code>sliding_cancel_minutes</code> is used. If <code>cancel_minutes</code> and <code>sliding_cancel_minutes</code> are both used then a job must run for <code>max(cancel_minutes + sampling_period_minutes, sliding_warning_minutes)</code> before a warning can be sent using the sliding window approach. After the warning is sent, the job can be cancelled <code>sliding_cancel_minutes</code> minus <code>sliding_warning_minutes</code> later. See <code>warning_frac</code> to learn how to send warnings and cancel jobs as soon as possible.</p> </li> <li> <p><code>sliding_cancel_minutes</code>: (Optional/Required) Cancel jobs found with 0% GPU utilization during a sliding time window of this number of minutes. This setting uses a sliding time window whereas <code>cancel_minutes</code> uses a fixed time window over the start of job. If <code>cancel_minutes</code> is also set then a job must run for at least <code>cancel_minutes</code> plus <code>sampling_period_minutes</code> plus the difference between <code>sliding_cancel_minutes</code> and <code>sliding_warning_minutes</code> before it can be cancelled by the sliding window approach. Users are guaranteed to receive a warning email before cancellation. One must set <code>cancel_minutes</code> and/or <code>sliding_cancel_minutes</code>.</p> </li> <li> <p><code>email_file_sliding_warning</code>: (Optional/Required) File to be used for the warning email. If <code>sliding_cancel_minutes</code> is set then this setting and <code>sliding_warning_minutes</code> are required.</p> </li> <li> <p><code>email_file_sliding_cancel</code>: (Optional/Required) File to be used for the cancellation email. This setting is required if <code>sliding_cancel_minutes</code> is set.</p> </li> <li> <p><code>email_subject</code>: (Optional) Subject of the email message to users.</p> </li> <li> <p><code>jobid_cache_path</code>: (Optional/Required) Path to a writable directory to store hidden cache files. Caching decreases the load on the Prometheus server. This setting is required if <code>sliding_cancel_minutes</code> is set. Use <code>ls -a</code> to see the hidden files.</p> </li> <li> <p><code>max_interactive_hours</code>: (Optional) An interactive job will not be cancelled if the run time limit is less than or equal to <code>max_interactive_hours</code> and the number of allocated GPUs is less than or equal to <code>max_interactive_gpus</code>. Remove these lines if interactive jobs should not receive special treatment. An interactive job is one with a <code>jobname</code> that starts with either <code>interactive</code> or <code>sys/dashboard</code>. If <code>max_interactive_hours</code> is specified then <code>max_interactive_gpus</code> is required.</p> </li> <li> <p><code>max_interactive_gpus</code>: (Optional) See <code>max_interactive_hours</code> above.</p> </li> <li> <p><code>gpu_frac_threshold</code>: (Optional) For a given job, let <code>g</code> be the ratio of the number of GPUs with non-zero utilization to the number of allocated GPUs. Jobs with <code>g</code> greater than or equal to  <code>gpu_frac_threshold</code> will be excluded. For example, if a job uses 7 of the 8 allocated GPUs and <code>gpu_frac_threshold</code> is 0.8 then it will be excluded from cancellation since 7/8 &gt; 0.8. This quantity varies between 0 and 1. Default: 1.0</p> </li> <li> <p><code>fraction_of_period</code>: (Optional) Fraction of the sampling period that can be used for querying the Prometheus server. The sampling period or <code>sampling_period_minutes</code> is the time between <code>cron</code> jobs for this alert. This setting imposes a limit on the amount of time spent on querying the server so that the code finishes before the next <code>cron</code> job. This quantity varies between 0 and 1 with the default being 0.5. If output such as <code>INFO: Only cached 42 of 100 jobs. Will try again on next call.</code> is repeatedly seen (excluding the starting period) then consider increasing <code>fraction_of_period</code> and/or <code>sampling_period_minutes</code>. If there are multiple entries for this alert then use a maximum value for this setting of less than 0.75 divided by the number of entries. Default: 0.5</p> </li> <li> <p><code>warning_frac</code>: (Optional) Fraction of <code>sliding_warning_minutes</code> that must pass before a job that was previously found to be using the GPUs will be re-examined for idle GPUs. This quantity varies between 0 and 1. The default value of 1.0 minimizes the load on the Prometheus server but it can allow jobs with idle GPUs to run for longer than necessary. To cancel jobs sooner, at the expense of more calls to Prometheus, use a smaller value such as 0.25 or 0.5. If <code>warning_frac: 0.5</code>and <code>sliding_warning_minutes: 240</code> then jobs that have been found to be using the GPU(s) at least once in the last 240 minutes will be checked again 120 minutes later. The product of <code>warning_frac</code> and <code>sliding_warning_minutes</code> should be much greater than <code>sampling_period_minutes</code>. Default: 1.0</p> </li> <li> <p><code>nodelist</code>: (Optional) Only apply this alert to jobs that ran on the specified nodes. This setting is only available for <code>cancel_minutes</code>. It will not work with <code>sliding_cancel_minutes</code>. See example.</p> </li> <li> <p><code>excluded_qos</code>: (Optional) List of QOSes to exclude from this alert.</p> </li> <li> <p><code>excluded_partitions</code>: (Optional) List of partitions to exclude from this alert. This is useful when <code>partitions: [\"*\"]</code> is used.</p> </li> <li> <p><code>excluded_users</code>: (Optional) List of usernames to exclude from this alert.</p> </li> <li> <p><code>do_not_cancel</code>: (Optional) If <code>True</code> then <code>scancel</code> will not be called. This is useful for testing only. In this case, one should call the alert with <code>--email --no-emails-to-users</code>. Default: <code>False</code></p> </li> <li> <p><code>warnings_to_admin</code>: (Optional) If <code>True</code> then warning emails (in addition to cancellation emails) will be sent to <code>admin_emails</code>. This is useful for testing. Default: <code>False</code></p> </li> <li> <p><code>admin_emails</code>: (Optional) List of administrator email addresses that should receive the warning and cancellation emails that are sent to users.</p> </li> </ul> <p>Times are Not Exact</p> <p>Jobs are not cancelled after exactly <code>cancel_minutes</code> or <code>sliding_cancel_minutes</code> since Slurm jobs can start at any time and the alert is only called every N minutes via <code>cron</code> or another scheduler. The same is true for warning emails.</p> <p>In Jobstats, a GPU is said to have 0% utilization if all of the measurements made by the NVIDIA exporter over a given time window are zero. Measurements are typically made every 30 seconds or so. For the actual value at your institution see <code>SAMPLING_PERIOD</code> in <code>config.py</code> for Jobstats.</p>"},{"location":"alert/cancel_gpu_jobs/#example-configurations","title":"Example Configurations","text":"<p>Jobs with GPUs that are idle for the first 2 hours (120 minutes) will be cancelled:</p> <pre><code>cancel-zero-gpu-jobs-1:\n  cluster:\n    - della\n  partitions:\n    - gpu\n    - llm\n  sampling_period_minutes: 15  # minutes\n  first_warning_minutes:   60  # minutes\n  scond_warning_minutes:  105  # minutes\n  cancel_minutes:         120  # minutes\n  email_file_first_warning:  \"cancel_gpu_jobs_warning_1.txt\"\n  email_file_second_warning: \"cancel_gpu_jobs_warning_2.txt\"\n  email_file_cancel:         \"cancel_gpu_jobs_scancel_3.txt\"\n  jobid_cache_path: /path/to/writable/directory/\n  admin_emails:\n    - admin@institution.edu\n</code></pre> <p>The configuration above will send warning emails after 60 and 105 minutes.</p> <p>The example below is the same as that above except only one warning email will be sent:</p> <pre><code>cancel-zero-gpu-jobs-1:\n  cluster:\n    - della\n  partitions:\n    - gpu\n    - llm\n  sampling_period_minutes: 15  # minutes\n  first_warning_minutes:   60  # minutes\n  cancel_minutes:         120  # minutes\n  email_file_first_warning: \"cancel_gpu_jobs_warning_1.txt\"\n  email_file_cancel:        \"cancel_gpu_jobs_scancel_3.txt\"\n  jobid_cache_path: /path/to/writable/directory/\n  admin_emails:\n    - admin@institution.edu\n</code></pre> <p>Cancel jobs that are found to have 0% GPU utilization over any time period of 5 hours (300 minutes):</p> <pre><code>cancel-zero-gpu-jobs-1:\n  cluster:\n    - della\n  partitions:\n    - gpu\n    - llm\n  sampling_period_minutes:  15  # minutes\n  sliding_warning_minutes: 240  # minutes\n  sliding_cancel_minutes:  300  # minutes\n  email_file_sliding_warning: \"cancel_gpu_jobs_sliding_warning.txt\"\n  email_file_sliding_cancel:  \"cancel_gpu_jobs_sliding_cancel.txt\"\n  jobid_cache_path: /path/to/writable/directory/\n  admin_emails:\n    - admin@institution.edu\n</code></pre> <p>For the entry above, the user would receive a warning email after 4 hours (240 minutes).</p> <p>The example that follows uses both cancellation methods. Jobs with GPUs that are idle for the first 2 hours (120 minutes) will be cancelled. Jobs with idle GPU(s) for 5 hours (300 minutes) during any period will be cancelled.</p> <pre><code>cancel-zero-gpu-jobs-1:\n  cluster: della\n  partitions:\n    - gpu\n    - llm\n  sampling_period_minutes: 15  # minutes\n  first_warning_minutes:   60  # minutes\n  second_warning_minutes: 105  # minutes\n  cancel_minutes:         120  # minutes\n  email_file_first_warning:  \"cancel_gpu_jobs_warning_1.txt\"\n  email_file_second_warning: \"cancel_gpu_jobs_warning_2.txt\"\n  email_file_cancel:         \"cancel_gpu_jobs_scancel_3.txt\"\n  sliding_warning_minutes: 240  # minutes\n  sliding_cancel_minutes:  300  # minutes\n  email_file_sliding_warning: \"cancel_gpu_jobs_sliding_warning.txt\"\n  email_file_sliding_cancel:  \"cancel_gpu_jobs_sliding_cancel.txt\"\n  jobid_cache_path: /path/to/writable/directory/\n  admin_emails:\n    - admin@institution.edu\n</code></pre>"},{"location":"alert/cancel_gpu_jobs/#testing","title":"Testing","text":"<p>For testing, be sure to use:</p> <pre><code>  do_not_cancel: True\n  warnings_to_admin: True\n</code></pre> <p>Additionally, add the <code>--no-emails-to-users</code> flag:</p> <pre><code>$ job_defense_shield --cancel-zero-gpu-jobs --email --no-emails-to-users\n</code></pre> <p>Learn more about email testing.</p>"},{"location":"alert/cancel_gpu_jobs/#first-warning-email-fixed-window-at-start-of-job","title":"First Warning Email (Fixed Window at Start of Job)","text":"<p>Below is an example email for the first warning (see <code>email/cancel_gpu_jobs_warning_1.txt</code>):</p> <pre><code>Hi Alan (aturing),\n\nYou have GPU job(s) that have been running for nearly 1 hour but appear to not\nbe using the GPU(s):\n\n     JobID    Cluster Partition  GPUs-Allocated  GPUs-Unused GPU-Util  Hours\n   60131148    della     gpu            4             4         0%       1  \n\nYour jobs will be AUTOMATICALLY CANCELLED if they are found to not be using the\nGPUs for 2 hours.\n\nPlease consider cancelling the job(s) listed above by using the \"scancel\"\ncommand:\n\n   $ scancel 60131148\n\nReplying to this automated email will open a support ticket with Research\nComputing.\n</code></pre>"},{"location":"alert/cancel_gpu_jobs/#placeholders","title":"Placeholders","text":"<p>The following placeholders can be used in the email file:</p> <ul> <li><code>&lt;GREETING&gt;</code>: The greeting generated by <code>greeting-method</code>.</li> <li><code>&lt;CLUSTER&gt;</code>: The cluster specified for the alert.</li> <li><code>&lt;PARTITIONS&gt;</code>: The partitions listed for the alert.</li> <li><code>&lt;SAMPLING&gt;</code>: The sampling period in minutes (<code>sampling_period_minutes</code>).</li> <li><code>&lt;MINUTES-1ST&gt;</code>: Number of minutes before the first warning is sent (<code>first_warning_minutes</code>).</li> <li><code>&lt;HOURS-1ST&gt;</code>: Number of hours before the first warning is sent.</li> <li><code>&lt;CANCEL-MIN&gt;</code>: Number of minutes a job must run for before being cancelled (<code>cancel_minutes</code>).</li> <li><code>&lt;CANCEL-HRS&gt;</code>: Number of hours a job must run for before being cancelled.</li> <li><code>&lt;TABLE&gt;</code>: Table of job data.</li> <li><code>&lt;JOBSTATS&gt;</code>: <code>jobstats</code> command for the first JobID (<code>$ jobstats 12345678</code>).</li> <li><code>&lt;SCANCEL&gt;</code>: <code>scancel</code> command for the first JobID (<code>$ scancel 12345678</code>).</li> </ul>"},{"location":"alert/cancel_gpu_jobs/#second-warning-email-fixed-window-at-start-of-job","title":"Second Warning Email (Fixed Window at Start of Job)","text":"<p>Below is an example email for the second warning (see <code>email/cancel_gpu_jobs_warning_2.txt</code>):</p> <pre><code>Hi Alan (aturing),\n\nThis is a second warning. The jobs below will be cancelled in about 15 minutes\nunless GPU activity is detected:\n\n     JobID    Cluster Partition  GPUs-Allocated  GPUs-Unused GPU-Util  Hours\n   60131148    della     gpu            4             4         0%      1.6  \n\nReplying to this automated email will open a support ticket with Research\nComputing.\n</code></pre>"},{"location":"alert/cancel_gpu_jobs/#placeholders_1","title":"Placeholders","text":"<p>The following placeholders can be used in the email file:</p> <ul> <li><code>&lt;GREETING&gt;</code>: The greeting generated by <code>greeting-method</code>.</li> <li><code>&lt;CLUSTER&gt;</code>: The cluster specified for the alert.</li> <li><code>&lt;PARTITIONS&gt;</code>: The partitions listed for the alert.</li> <li><code>&lt;SAMPLING&gt;</code>: The sampling period in minutes (<code>sampling_period_minutes</code>).</li> <li><code>&lt;MINUTES-1ST&gt;</code>: Number of minutes before the first warning is sent (<code>first_warning_minutes</code>).</li> <li><code>&lt;MINUTES-2ND&gt;</code>: Number of minutes before the second warning is sent (<code>second_warning_minutes</code>).</li> <li><code>&lt;CANCEL-MIN&gt;</code>: Number of minutes a job must run for before being cancelled (<code>cancel_minutes</code>).</li> <li><code>&lt;CANCEL-HRS&gt;</code>: Number of hours a job must run for before being cancelled.</li> <li><code>&lt;TABLE&gt;</code>: Table of job data.</li> <li><code>&lt;JOBSTATS&gt;</code>: <code>jobstats</code> command for the first JobID (<code>$ jobstats 12345678</code>).</li> <li><code>&lt;SCANCEL&gt;</code>: <code>scancel</code> command for the first JobID (<code>$ scancel 12345678</code>).</li> </ul>"},{"location":"alert/cancel_gpu_jobs/#cancellation-email-fixed-window-at-start-of-job","title":"Cancellation Email (Fixed Window at Start of Job)","text":"<p>Below is an example email (see <code>email/cancel_gpu_jobs_scancel_3.txt</code>):</p> <pre><code>Hi Alan (aturing),\n\nThe jobs below have been cancelled because they ran for more than 2 hours at\n0% GPU utilization:\n\n     JobID   Cluster  Partition    State    GPUs-Allocated GPU-Util  Hours\n   60131148   della      gpu     CANCELLED         4          0%      2.1\n\nSee our GPU Computing webpage for three common reasons for encountering zero GPU\nutilization:\n\n    https://your-institution.edu/knowledge-base/gpu-computing\n\nReplying to this automated email will open a support ticket with Research\nComputing.\n</code></pre>"},{"location":"alert/cancel_gpu_jobs/#placeholders_2","title":"Placeholders","text":"<p>The following placeholders can be used in the email file:</p> <ul> <li><code>&lt;GREETING&gt;</code>: The greeting generated by <code>greeting-method</code>.</li> <li><code>&lt;CLUSTER&gt;</code>: The cluster specified for the alert.</li> <li><code>&lt;PARTITIONS&gt;</code>: The partitions listed for the alert.</li> <li><code>&lt;SAMPLING&gt;</code>: The sampling period in minutes (<code>sampling_period_minutes</code>).</li> <li><code>&lt;CANCEL-MIN&gt;</code>: Number of minutes a job must run for before being cancelled (<code>cancel_minutes</code>).</li> <li><code>&lt;CANCEL-HRS&gt;</code>: Number of hours a job must run for before being cancelled (<code>cancel_minutes</code>/60).</li> <li><code>&lt;TABLE&gt;</code>: Table of job data.</li> <li><code>&lt;JOBSTATS&gt;</code>: <code>jobstats</code> command for the first JobID (<code>$ jobstats 12345678</code>).</li> <li><code>&lt;SCANCEL&gt;</code>: <code>scancel</code> command for the first JobID (<code>$ scancel 12345678</code>).</li> </ul>"},{"location":"alert/cancel_gpu_jobs/#warning-email-sliding-window-over-last-n-minutes","title":"Warning Email (Sliding Window Over Last N Minutes)","text":"<p>Below is an example email for the warning (see <code>email/cancel_gpu_sliding_warning.txt</code>):</p> <pre><code>Hi Alan (aturing),\n\nYou have job(s) that have not used the GPU(s) for the last 3 hours:\n\n     JobID   Cluster  Partition  State   GPUs GPUs-Unused GPU-Util  Hours\n   60131148   della      gpu    RUNNING    4       4         0%       3\n\nYour jobs will be AUTOMATICALLY CANCELLED if they are found to not be using the\nGPUs for a period of 4 hours.\n\nPlease consider cancelling the job(s) listed above by using the \"scancel\"\ncommand:\n\n   $ scancel 60131148\n\nReplying to this automated email will open a support ticket with Research\nComputing.\n</code></pre>"},{"location":"alert/cancel_gpu_jobs/#placeholders_3","title":"Placeholders","text":"<p>The following placeholders can be used in the email file:</p> <ul> <li><code>&lt;GREETING&gt;</code>: The greeting generated by <code>greeting-method</code>.</li> <li><code>&lt;CLUSTER&gt;</code>: The cluster specified for the alert.</li> <li><code>&lt;PARTITIONS&gt;</code>: A comma-separated list of partitions used by the user.</li> <li><code>&lt;SAMPLING&gt;</code>: The sampling period in minutes (<code>sampling_period_minutes</code>).</li> <li><code>&lt;WARNING-MIN&gt;</code>: Number of minutes before the warning email is sent (<code>sliding_warning_minutes</code>).</li> <li><code>&lt;WARNING-HRS&gt;</code>: Number of hours before the warning email is sent (<code>sliding_warning_minutes</code>/60).</li> <li><code>&lt;CANCEL-MIN&gt;</code>: Time period in minutes with 0% GPU utilization for a job to be cancelled (<code>sliding_cancel_minutes</code>).</li> <li><code>&lt;CANCEL-HRS&gt;</code>: Time period in hours with 0% GPU utilization for a job to be cancelled (<code>sliding_cancel_minutes</code>/60).</li> <li><code>&lt;TABLE&gt;</code>: Table of job data.</li> <li><code>&lt;JOBSTATS&gt;</code>: <code>jobstats</code> command for the first JobID (<code>$ jobstats 12345678</code>).</li> <li><code>&lt;SCANCEL&gt;</code>: <code>scancel</code> command for the first JobID (<code>$ scancel 12345678</code>).</li> </ul>"},{"location":"alert/cancel_gpu_jobs/#cancellation-email-sliding-window-over-last-n-minutes","title":"Cancellation Email (Sliding Window Over Last N Minutes)","text":"<p>Below is an example email (see <code>email/cancel_gpu_jobs_sliding_cancel.txt</code>):</p> <pre><code>Hi Alan (aturing),\n\nThe jobs below have been cancelled because they did not use the GPU(s) for the\nlast 4 hours:\n\n     JobID  Cluster  Partition   State    GPUs  GPUs-Unused  GPU-Util  Hours\n   60131148  della      gpu    CANCELLED    4        4          0%       4\n\nSee our GPU Computing webpage for three common reasons for encountering zero GPU\nutilization:\n\n    https://your-institution.edu/knowledge-base/gpu-computing\n\nReplying to this automated email will open a support ticket with Research\nComputing.\n</code></pre>"},{"location":"alert/cancel_gpu_jobs/#placeholders_4","title":"Placeholders","text":"<p>The following placeholders can be used in the email file:</p> <ul> <li><code>&lt;GREETING&gt;</code>: The greeting generated by <code>greeting-method</code>.</li> <li><code>&lt;CLUSTER&gt;</code>: The cluster specified for the alert (i.e., <code>cluster</code>).</li> <li><code>&lt;CANCEL-MIN&gt;</code>: Time period in minutes with 0% GPU utilization for a job to be cancelled (<code>sliding_cancel_minutes</code>).</li> <li><code>&lt;CANCEL-HRS&gt;</code>: Time period in hours with 0% GPU utilization for a job to be cancelled (<code>sliding_cancel_minutes</code>/60).</li> <li><code>&lt;TABLE&gt;</code>: Table of job data.</li> <li><code>&lt;JOBSTATS&gt;</code>: <code>jobstats</code> command for the first JobID (<code>$ jobstats 12345678</code>).</li> <li><code>&lt;SCANCEL&gt;</code>: <code>scancel</code> command for the first JobID (<code>$ scancel 12345678</code>).</li> </ul>"},{"location":"alert/cancel_gpu_jobs/#cron","title":"<code>cron</code>","text":"<p>Below is an example entry for <code>crontab</code>:</p> <pre><code>*/15 * * * * /path/to/job_defense_shield --cancel-zero-gpu-jobs --email -M della -r gpu &gt; /path/to/log/zero_gpu_utilization.log 2&gt;&amp;1\n</code></pre> <p>Note that the alert is ran every 15 minutes. This must also be the value of <code>sampling_period_minutes</code>.</p>"},{"location":"alert/cancel_gpu_jobs/#report","title":"Report","text":"<p>There is no report for this alert. To find out which users have the most GPU-hours at 0% utilization, see this alert. If you are automatically cancelling GPU jobs then no users should be able to waste significant resources.</p>"},{"location":"alert/cancel_gpu_jobs/#other-projects","title":"Other Projects","text":"<p>One can also automatically cancel GPU jobs using the HPC Dashboard by Arizona State University.</p>"},{"location":"alert/cpu_fragmentation/","title":"Multinode CPU Fragmentation","text":"<p>This alert identifies CPU jobs that are using too many nodes or too few CPU-cores per node.</p> <p>Consider a cluster with 64 CPU-cores per node. A user can run a job that requires 128 CPU-cores by (1) allocating 64 CPU-cores on 2 nodes or (2) allocating 4 CPU-cores on 32 nodes. The former is in general strongly preferred. This alert catches jobs doing the latter, i.e., multinode jobs that allocate less than the number of available CPU-cores per node (e.g., 4 CPU-cores on 32 nodes). The memory usage of each job is taken into account when looking for fragmentation.</p> <p>Jobs with 0% CPU utilization on a node are ignored since those are captured by a another alert.</p>"},{"location":"alert/cpu_fragmentation/#configuration-file","title":"Configuration File","text":"<p>Below is an example alert entry for <code>config.yaml</code>:</p> <pre><code>multinode-cpu-fragmentation-1:\n  cluster: della\n  partitions:\n    - cpu\n  min_run_time:     61  # minutes\n  cores_per_node:   32  # count\n  cores_fraction:  0.8  # [0.0, 1.0]\n  mem_per_node:    190  # GB\n  safety_fraction: 0.2  # [0.0, 1.0]\n  email_file: \"multinode_cpu_fragmentation.txt\"\n  admin_emails:\n    - admin@institution.edu\n</code></pre> <p>The parameters are explained below:</p> <ul> <li> <p><code>cluster</code>: Specify the cluster name as it appears in the Slurm database.</p> </li> <li> <p><code>partitions</code>: Specify one or more Slurm partitions. Use <code>\"*\"</code> to include all partitions (i.e., <code>partitions: [\"*\"]</code>).</p> </li> <li> <p><code>cores_per_node</code>: CPU-cores per node.</p> </li> <li> <p><code>cores_fraction</code>: The ratio of allocated CPU-cores to the product of the number of nodes and CPU-cores per node. Jobs that use greater than the value of <code>cores_fraction</code> will be ignored. This quantity varies between 0 and 1.</p> </li> <li> <p><code>mem_per_node</code>: CPU memory per node in units of GB.</p> </li> <li> <p><code>safety_frac</code>: The memory used by the job is multiplied by 1 + <code>safety_frac</code> and this number is compared against the product of the number of nodes and the memory per node in deciding whether or not sufficent memory was used to ignore the job independent of the number of allocated CPU-cores. The idea is to ignore jobs that are almost using all of the allocated CPU memory.</p> </li> <li> <p><code>email_file</code>: The text file to be used for the email message to users.</p> </li> <li> <p><code>min_nodes_thres</code>: (Optional) Minimum number of allocated nodes for a job to be considered. For instance, if <code>min_nodes_thres: 4</code> then jobs that ran on 3 nodes or less will be ignored. Default: 2</p> </li> <li> <p><code>cores_per_node_thres</code>: (Optional) Only consider jobs with less than this number of cores per node. If this setting is used then the following settings will be ignored: <code>cores_per_node</code>, <code>mem_per_node</code>, and <code>safety_frac</code>. Additionally, the only placeholders that will be available are <code>&lt;GREETING&gt;</code>, <code>&lt;DAYS&gt;</code>, <code>&lt;CLUSTER&gt;</code>, <code>&lt;PARTITIONS&gt;</code>, <code>&lt;TABLE&gt;</code>, and <code>&lt;JOBSTATS&gt;</code>. The <code>&lt;TABLE&gt;</code> placeholder will not contain <code>Min-Nodes</code>. The <code>cores_per_node_thres</code> setting provides a simple way to address multinode CPU fragmentation on a cluster composed of hetergeneous nodes.</p> </li> <li> <p><code>min_run_time</code>: (Optional) Minimum run time of a job in units of minutes. If <code>min_run_time: 61</code> then jobs that ran for an hour or less are ignored. Default: 0</p> </li> <li> <p><code>excluded_qos</code>: (Optional) List of QOSes to exclude from this alert.</p> </li> <li> <p><code>excluded_partitions</code>: (Optional) List of partitions to exclude from this alert. This is useful when <code>partitions: [\"*\"]</code> is used.</p> </li> <li> <p><code>excluded_users</code>: (Optional) List of usernames to exclude from the alert.</p> </li> <li> <p><code>admin_emails</code>: (Optional) List of administrator email addresses that should receive copies of the emails that are sent to users.</p> </li> <li> <p><code>email_subject</code>: (Optional) Subject of the email message to users.</p> </li> <li> <p><code>report_title</code>: (Optional) Title of the report to system administrators.</p> </li> </ul> <p>Below is an entry appropriate for a heterogeneous cluster:</p> <pre><code>multinode-cpu-fragmentation-1:\n  cluster: della\n  partitions:\n    - cpu\n  min_run_time:         61  # minutes\n  cores_per_node_thres: 16  # count\n  email_file: \"multinode_cpu_fragmentation.txt\"\n  admin_emails:\n    - admin@institution.edu\n</code></pre> <p>When <code>cores_per_node_thres</code> is used, other settings are ignored and a limited number of placeholders are available for creating the email message.</p>"},{"location":"alert/cpu_fragmentation/#report-for-system-administrators","title":"Report for System Administrators","text":"<p>Below is an example report:</p> <pre><code>$ job_defense_shield --multinode-cpu-fragmentation\n\n                     Multinode CPU Jobs with Fragmentation                         \n-------------------------------------------------------------------------------\n JobID    User   Nodes  Cores Mem-per-Node-Used Cores-per-Node Min-Nodes Emails\n-------------------------------------------------------------------------------\n6286517  u45923   20     20          1 GB             1            1        0   \n6286840  u45923   10     10          1 GB             1            1        0   \n6287417  u45923   10     10          3 GB             1            1        0   \n6288471  u45923   10     10          4 GB             1            1        0   \n6289852  u45923    5     10         12 GB             2            1        0   \n-------------------------------------------------------------------------------\n   Cluster: della\nPartitions: cpu\n     Start: Wed Mar 12, 2025 at 11:44 AM\n       End: Wed Mar 19, 2025 at 11:44 AM\n</code></pre> <p>The <code>Min-Nodes</code> field is calculated based on the hardware specifications and the number of CPU-cores allocated by the user. All of the jobs in the table above could have ran on one node.</p>"},{"location":"alert/cpu_fragmentation/#email-message-to-users","title":"Email Message to Users","text":"<p>Below is an example message (see <code>email/multinode_cpu_fragmentation.txt</code>):</p> <pre><code>Hello Alan (u45923),\n\nBelow are your jobs over the past 7 days on Della which appear to be using\nmore nodes than necessary:\n\n     JobID   Nodes  Mem-per-Node  Cores-per-Node  Hours  Nodes-Needed\n    62862517   20        1 GB            1         2.2        1      \n    62869840   10        1 GB            1         2.9        1      \n    62874417   10        3 GB            1          12        1      \n    62886471   10        4 GB            1          12        1      \n    62892852    5       12 GB            2          22        1      \n\nThe \"Nodes\" column shows the number of nodes used to run the job. The\n\"Nodes-Needed\" column shows the minimum number of nodes needed to run the\njob (these values are calculated based on the number of requested CPU-cores\nwhile taking into account the CPU memory usage of the job). \"Mem-per-Node\"\nis the mean CPU memory used per node.\n\nReplying to this automated email will open a support ticket with Research\nComputing.\n</code></pre>"},{"location":"alert/cpu_fragmentation/#placeholders","title":"Placeholders","text":"<p>The following placeholders can be used in the email file:</p> <ul> <li><code>&lt;GREETING&gt;</code>: Greeting generated by <code>greeting_method</code>.</li> <li><code>&lt;CLUSTER&gt;</code>: The name of the cluster.</li> <li><code>&lt;PARTITIONS&gt;</code>: A comma-separated list of partitions used by the user.</li> <li><code>&lt;DAYS&gt;</code>: Number of days in the time window (default is 7).</li> <li><code>&lt;CPN&gt;</code>: Number of CPU-cores per node (i.e., <code>cores_per_node</code>).</li> <li><code>&lt;MPN&gt;</code>: CPU memory per node (i.e., <code>mem_per_node</code>).</li> <li><code>&lt;TABLE&gt;</code>: A table of jobs of the user.</li> <li><code>&lt;NUM-CORES&gt;</code>: Product of minimum number of nodes needed and the number CPU-cores per node.</li> </ul> <p>Note that if <code>cores_per_node_thres</code> is defined then only a limited number of placeholders are available.</p>"},{"location":"alert/cpu_fragmentation/#usage","title":"Usage","text":"<p>Generate a report for system administrators:</p> <pre><code>$ job_defense_shield --multinode-cpu-fragmentation --email\n</code></pre> <p>Send emails to the offending users:</p> <pre><code>$ job_defense_shield --multinode-cpu-fragmentation --email\n</code></pre> <p>See which users have received emails and when:</p> <pre><code>$ job_defense_shield --multinode-cpu-fragmentation --check\n</code></pre>"},{"location":"alert/cpu_fragmentation/#cron","title":"cron","text":"<pre><code>0 9 * * 1-5 /path/to/job_defense_shield --multinode-cpu-fragmentation --email -M della -r cpu &gt; /path/to/log/cpu_fragmentation.log 2&gt;&amp;1\n</code></pre>"},{"location":"alert/excess_cpu_mem_per_gpu/","title":"Too Much Allocated CPU Memory per GPU","text":"<p>This alert identifies jobs that allocate too much CPU memory per GPU. The goal is to prevent the situation where there are free GPUs on a node but not enough CPU memory to accept new jobs.</p>"},{"location":"alert/excess_cpu_mem_per_gpu/#configuration-file","title":"Configuration File","text":"<p>Below is an example entry for <code>config.yaml</code>:</p> <pre><code>too-much-cpu-mem-per-gpu-1:\n  cluster: della\n  partitions:\n    - gpu\n    - llm\n  cores_per_node: 96           # count\n  gpus_per_node: 8             # count\n  cpu_mem_per_node: 1000       # GB\n  cpu_mem_per_gpu_target: 115  # GB\n  cpu_mem_per_gpu_limit: 128   # GB\n  mem_eff_thres: 0.8           # [0.0, 1.0]\n  min_run_time: 30             # minutes\n  email_file: \"too_much_cpu_mem_per_gpu.txt\"\n  admin_emails:\n    - admin@institution.edu\n</code></pre> <p>The available settings are listed below:</p> <ul> <li> <p><code>cluster</code>: Specify the cluster name as it appears in the Slurm database.</p> </li> <li> <p><code>partitions</code>: Specify one or more Slurm partitions. Use <code>\"*\"</code> to include all partitions (i.e., <code>partitions: [\"*\"]</code>).</p> </li> <li> <p><code>cores_per_node</code>: Number of CPU-cores per node.</p> </li> <li> <p><code>gpus_per_node</code>: Number of GPUs per node.</p> </li> <li> <p><code>cpu_mem_per_node</code>: Total CPU memory per node in units of GB.</p> </li> <li> <p><code>cpu_mem_per_gpu_target</code>: This should a value slightly less than the total CPU memory divided by the number of GPUs per node in GB. For instance, nodes with 1000 GB of memory and 8 GPUs might use <code>cpu_mem_per_gpu_target: 120</code>. The idea is to save some memory for the operating system. This setting may be interpreted as a soft limit.</p> </li> <li> <p><code>cpu_mem_per_gpu_limit</code>: Identify jobs with a CPU memory allocation per GPU greater than this value. It is reasonable to use the same value as <code>cpu_mem_per_gpu_target</code> for this setting.</p> </li> <li> <p><code>email_file</code>: The text file to be used for the email message to users.</p> </li> </ul> <p><code>gpu_hours_threshold</code>: (Optional) Minimum number of GPU-hours (summed over the jobs) for the user to be considered. This setting makes it possible to ignore users that are not consuming many resources. Default: 0</p> <ul> <li> <p><code>min_run_time</code>: (Optional) Minimum run time in minutes for a job to be included in the calculation. For example, if <code>min_run_time: 30</code> is used then jobs that ran for less than 30 minutes are ignored. Default: 0</p> </li> <li> <p><code>mem_eff_thres</code>: (Optional) Ignore jobs where the ratio of used to allocated CPU memory is greater than or equal to this value. Default: 1.0</p> </li> <li> <p><code>include_running_jobs</code>: (Optional) If <code>True</code> then jobs in a state of <code>RUNNING</code> will be included in the calculation. The Prometheus server must be queried for each running job, which can be an expensive operation. Default: False</p> </li> <li> <p><code>nodelist</code>: (Optional) Only apply this alert to jobs that ran on the specified nodes. See example.</p> </li> <li> <p><code>excluded_qos</code>: (Optional) List of QOSes to exclude from this alert.</p> </li> <li> <p><code>excluded_partitions</code>: (Optional) List of partitions to exclude from this alert. This is useful when <code>partitions: [\"*\"]</code> is used.</p> </li> <li> <p><code>excluded_users</code>: (Optional) List of usernames to exclude from receiving emails.</p> </li> <li> <p><code>admin_emails</code>: (Optional) List of administrator email addresses that should receive copies of the emails that are sent to users.</p> </li> <li> <p><code>email_subject</code>: (Optional) Subject of the email message to users.</p> </li> <li> <p><code>report_title</code>: (Optional) Title of the report to system administrators.</p> </li> </ul>"},{"location":"alert/excess_cpu_mem_per_gpu/#report-for-system-administrators","title":"Report for System Administrators","text":"<p>An example report is shown below:</p> <pre><code>$ job_defense_shield --too-much-cpu-mem-per-gpu\n\n                         Too Much CPU Memory Per GPU                          \n-------------------------------------------------------------------------------\n JobID    User  Hours Mem-Eff GPUs CPU-Mem-per-GPU CPU-Mem-per-GPU-Limit Emails\n-------------------------------------------------------------------------------\n6279176  u29427   23     8%     1       500 GB            240 GB          1 (4)\n6279179  u29427   31     2%     1       500 GB            240 GB          1 (4)\n6270434  u15404   48     6%     1       512 GB            240 GB          3 (2)\n6293177  u15404  1.4     1%     1       512 GB            240 GB          3 (2)\n6291411  u81448  3.8     0%     1       512 GB            240 GB          0    \n6283444  u35452  1.8     0%     1       500 GB            240 GB          1 (5)\n-------------------------------------------------------------------------------\n   Cluster: della\nPartitions: gpu\n     Start: Sun Mar 09, 2025 at 10:54 PM\n       End: Sun Mar 16, 2025 at 10:54 PM\n</code></pre>"},{"location":"alert/excess_cpu_mem_per_gpu/#email-message-to-users","title":"Email Message to Users","text":"<p>Below is an example email (see <code>email/too_much_cpu_mem_per_gpu.txt</code>):</p> <pre><code>Hello Alan (u12345),\n\nYour Della (PLI) jobs appear to be allocating more CPU memory than necessary:\n\n     JobID   Hours Mem-Eff CPU-Mem  GPUs CPU-Mem-per-GPU CPU-Mem-per-GPU-Limit\n    62733079  1.3    17%    512 GB   2        256 GB             115 GB       \n    62735106  1.4    12%    512 GB   2        256 GB             115 GB       \n\nEach node on Della (PLI) has 1000 GB of CPU memory and 8 GPUs. If possible\nplease only allocate up to the soft limit of 115 GB of CPU memory per GPU. This\nwill prevent the situation where there are free GPUs on a node but not enough\nCPU memory to accept new jobs.\n\n\"Mem-Eff\" is the memory efficiency or the ratio of used to allocated CPU memory.\nA good target value for this quantity is 80% and above. Please use an accurate\nvalue for the --mem, --mem-per-cpu or --mem-per-gpu Slurm directive. For job\n62733079, one could have used:\n\n    #SBATCH --mem-per-gpu=50G\n\nReplying to this automated email will open a support ticket with Research\nComputing.\n</code></pre>"},{"location":"alert/excess_cpu_mem_per_gpu/#placeholders","title":"Placeholders","text":"<p>The following placeholders can be used in the email file:</p> <ul> <li><code>&lt;GREETING&gt;</code>: The greeting generated by <code>greeting-method</code>.</li> <li><code>&lt;CLUSTER&gt;</code>: The cluster specified for the alert.</li> <li><code>&lt;PARTITIONS&gt;</code>: A comma-separated list of partitions used by the user.</li> <li><code>&lt;CORES&gt;</code>: Cores per node (i.e., <code>cores_per_node</code>).</li> <li><code>&lt;MEMORY&gt;</code>: CPU memory per node (i.e., <code>cpu_mem_per_node</code>).</li> <li><code>&lt;MEM-PER-GPU&gt;</code>: Suggested CPU memory per GPU in units of GB for the first job of the user. This is calculated as the max of either 8 GB or 1.2 times the CPU memory usage per GPU of the first job of the user.</li> <li><code>&lt;GPUS&gt;</code>: GPUs per node (i.e., <code>gpus_per_node</code>).</li> <li><code>&lt;TARGET&gt;</code>: The soft limit for CPU memory per GPU in GB (i.e., <code>cpu_mem_per_gpu_target</code>).</li> <li><code>&lt;DAYS&gt;</code>: Number of days in the time window (default is 7).</li> <li><code>&lt;TABLE&gt;</code>: Table of job data.</li> <li><code>&lt;JOBSTATS&gt;</code>: The <code>jobstats</code> command for the first job of the user.</li> <li><code>&lt;JOBID&gt;</code>: The <code>JobID</code> of the first job of the user.</li> </ul>"},{"location":"alert/excess_cpu_mem_per_gpu/#usage","title":"Usage","text":"<p>Generate a report of the jobs allocating too much CPU memory per GPU:</p> <pre><code>$ job_defense_shield --too-much-cpu-mem-per-gpu\n</code></pre> <p>Email users about allocating too much CPU memory per GPU:</p> <pre><code>$ job_defense_shield --too-much-cpu-mem-per-gpu --email\n</code></pre> <p>See which users have received emails and when:</p> <pre><code>$ job_defense_shield --too-much-cpu-mem-per-gpu --check\n</code></pre>"},{"location":"alert/excess_cpu_mem_per_gpu/#cron","title":"cron","text":"<p>Below is an example entry for <code>crontab</code>:</p> <pre><code>0 9 * * * /path/to/job_defense_shield --too-much-cpu-mem-per-gpu --email &gt; /path/to/log/too_much_cpu_mem_per_gpu.log 2&gt;&amp;1\n</code></pre>"},{"location":"alert/excess_cpu_memory/","title":"Over-Allocating CPU Memory","text":"<p>This alert identifies users that are over-allocating CPU memory.</p> <p>CPU Only</p> <p>This alert is for CPU jobs. For GPU jobs, see this alert.</p>"},{"location":"alert/excess_cpu_memory/#configuration-file","title":"Configuration File","text":"<p>Below is an example entry for <code>config.yaml</code>:</p> <pre><code>excess-cpu-memory-1:\n  cluster: della\n  partitions:\n    - cpu\n  min_run_time:             61  # minutes\n  tb_hours_threshold:       50  # terabyte-hours\n  ratio_threshold:        0.35  # [0.0, 1.0]\n  mean_ratio_threshold:   0.35  # [0.0, 1.0]\n  median_ratio_threshold: 0.35  # [0.0, 1.0]\n  num_top_users:            10  # count\n  email_file: \"excess_cpu_memory.txt\"\n  admin_emails:\n    - admin@institution.edu\n</code></pre> <p>Each configuration parameter is explained below:</p> <ul> <li> <p><code>cluster</code>: Specify the cluster name as it appears in the Slurm database.</p> </li> <li> <p><code>partitions</code>: Specify one or more Slurm partitions. Use <code>\"*\"</code> to include all partitions (i.e., <code>partitions: [\"*\"]</code>).</p> </li> <li> <p><code>tb_hours_threshold</code>: The threshold value for (unused) memory-hours in units of TB-hours. A user must have this number or above to receive an email message. This quantity is the unused CPU memory of a job multiplied by the elapsed time (summed over all jobs).</p> </li> <li> <p><code>ratio_threshold</code>: This quantity is the sum of CPU memory used divded by the total memory allocated for all jobs of the user. This quantity varies between 0 and 1.</p> </li> <li> <p><code>median_ratio_threshold</code>: The median value of memory used divided by memory allocated for the individual jobs of the user. This quantity varies between 0 and 1.</p> </li> <li> <p><code>mean_ratio_threshold</code>: The mean value of memory used divided by memory allocated for the individual jobs for a given user. This quantity varies between 0 and 1.</p> </li> <li> <p><code>email_file</code>: The text file to be used for the email message to users.</p> </li> <li> <p><code>min_run_time</code>: (Optional) Minimum run time of a job in units of minutes. If <code>min_run_time: 61</code> then jobs that ran for an hour or less are ignored. Default: 0</p> </li> <li> <p><code>cores_per_node</code>: (Optional) Number of CPU-cores per node.</p> </li> <li> <p><code>mem_per_node</code>: (Optional) CPU memory per node in units of GB.</p> </li> <li> <p><code>cores_fraction</code>: (Optional) Fraction of the cores per node that will cause the job to be ignored. For instance, with <code>cores_fraction: 0.8</code> if a node has 32 cores and the job allocates 30 of them then the job will be ignored since 30/32 &gt; 0.8. Default: 1</p> </li> <li> <p><code>num_top_users</code>: (Optional) Only consider the number of users equal to this value after sorting by \"unused TB-hours\". Default: 15</p> </li> <li> <p><code>show_all_users</code>: (Optional) Flag to show all of the top users in the report instead of only the top users with low memory usage. Default: False</p> </li> <li> <p><code>num_jobs_display</code>: (Optional) Number of jobs to display in the email message to users. Default: 10</p> </li> <li> <p><code>include_running_jobs</code>: (Optional) If <code>True</code> then jobs in a state of <code>RUNNING</code> will be included in the calculation. The Prometheus server must be queried for each running job, which can be an expensive operation. Default: False</p> </li> <li> <p><code>nodelist</code>: (Optional) Only apply this alert to jobs that ran on the specified nodes. See example.</p> </li> <li> <p><code>excluded_qos</code>: (Optional) List of QOSes to exclude from this alert.</p> </li> <li> <p><code>excluded_partitions</code>: (Optional) List of partitions to exclude from this alert. This is useful when <code>partitions: [\"*\"]</code> is used.</p> </li> <li> <p><code>excluded_users</code>: (Optional) List of usernames to exclude from the alert.</p> </li> <li> <p><code>admin_emails</code>: (Optional) List of administrator email addresses that should receive copies of the emails that are sent to users.</p> </li> <li> <p><code>email_subject</code>: (Optional) Subject of the email message to users.</p> </li> <li> <p><code>report_title</code>: (Optional) Title of the report to system administrators.</p> </li> </ul> <p>Info</p> <p>When <code>cores_per_node</code> and <code>mem_per_node</code> are defined, only jobs using more memory per core than <code>mem_per_node</code> divided by <code>cores_per_node</code> are included. For instance, if a node provides 64 cores and 512 GB of memory, only jobs allocating more than 8 GB/core are considered.</p>"},{"location":"alert/excess_cpu_memory/#report-for-system-administrators","title":"Report for System Administrators","text":"<p>Below is an example report:</p> <pre><code>$ job_defense_shield --excess-cpu-memory\n\n                         Users Allocating Excess CPU Memory                         \n------------------------------------------------------------------------------\n User   Unused    Used    Ratio  Ratio   Ratio Proportion CPU-Hrs  Jobs Emails\n       (TB-Hrs) (TB-Hrs) Overall  Mean  Median                                \n------------------------------------------------------------------------------\nu34981    164      163     0.50   0.50   0.54     0.19      8370    310  0  \nu76237    135        0     0.00   0.00   0.00     0.08      5628    243  1 (5)\nu63098     90        1     0.02   0.02   0.02     0.05       730     22  3 (1)\nu26174     71      189     0.73   0.72   0.71     0.15      7425    551  0  \nu89812     51       83     0.62   0.61   0.62     0.08      4023   2040  0  \n------------------------------------------------------------------------------\n   Cluster: della\nPartitions: cpu\n     Start: Tue Mar 11, 2025 at 11:45 AM\n       End: Tue Mar 18, 2025 at 11:45 AM\n</code></pre>"},{"location":"alert/excess_cpu_memory/#email-message-to-users","title":"Email Message to Users","text":"<p>Below is an example email (see <code>email/excess_cpu_memory.txt</code>):</p> <pre><code>Hello Alan (u34981),\n\nBelow are 10 of your 1725 jobs that ran on della (cpu) in the past 7 days:\n\n     JobID   Memory-Used Memory-Allocated Percent-Used  Cores  Hours\n    62623577     0 GB         20 GB            0%        1     16.4 \n    62626492     1 GB         20 GB            5%        1     15.5 \n    62626494     1 GB         20 GB            5%        1     15.5 \n    62626495     1 GB         20 GB            5%        1     15.5 \n\nIt appears that you are requesting too much CPU memory for your jobs since you\nare only using on average 3% of the allocated memory (for the 1725 jobs). This\nhas resulted in 161 TB-hours of unused memory which is equivalent to making\n5 nodes unavailable to all users (including yourself) for one week! A TB-hour is\nthe allocation of 1 terabyte of memory for 1 hour.\n\nReplying to this automated email will open a support ticket with Research\nComputing.\n</code></pre>"},{"location":"alert/excess_cpu_memory/#placeholders","title":"Placeholders","text":"<p>The following placeholders can be used in the email file:</p> <ul> <li><code>&lt;GREETING&gt;</code>: The greeting generated by <code>greeting-method</code>.</li> <li><code>&lt;CLUSTER&gt;</code>: The cluster specified for the alert (i.e., <code>cluster</code>).</li> <li><code>&lt;PARTITIONS&gt;</code>: A comma-separated list of the partitions used by the user.</li> <li><code>&lt;DAYS&gt;</code>: Time window of the alert in days (7 is default).</li> <li><code>&lt;CASE&gt;</code>: The rank of the user (see email file).</li> <li><code>&lt;NUM-JOBS&gt;</code>: Number of jobs that are over-allocating memory.</li> <li><code>&lt;UNUSED&gt;</code>: Memory-hours that were unused.</li> <li><code>&lt;PERCENT&gt;</code>: Mean memory efficiency or average ratio of used CPU memory to allocated.</li> <li><code>&lt;TABLE&gt;</code>: Table of job data.</li> <li><code>&lt;JOBSTATS&gt;</code>: The <code>jobstats</code> command for the first job of the user.</li> </ul> <p>When <code>mem_per_node</code> and <code>cores_per_node</code> are used then one more placeholder is available:</p> <ul> <li><code>&lt;NUM-WASTED-NODES&gt;</code>: The number of wasted nodes due to the wasted CPU memory. This is equal to the number of unused TB-hours divided by the product of the CPU memory per node in TB and the number of hours in the time window (default is 168 hours or 7 days).</li> </ul>"},{"location":"alert/excess_cpu_memory/#usage","title":"Usage","text":"<p>Generate a report for system administrators:</p> <pre><code>$ job_defense_shield --excess-cpu-memory\n</code></pre> <p>Send emails to the offending users:</p> <pre><code>$ job_defense_shield --excess-cpu-memory --email\n</code></pre> <p>See which users have received emails and when:</p> <pre><code>$ job_defense_shield --excess-cpu-memory --check\n</code></pre>"},{"location":"alert/excess_cpu_memory/#cron","title":"cron","text":"<p>Below is an example <code>crontab</code> entry:</p> <pre><code>0 9 * * 1-5 /path/to/job_defense_shield --excess-cpu-memory --email &gt; /path/to/log/excess_cpu_memory.log 2&gt;&amp;1\n</code></pre>"},{"location":"alert/gpu_fragmentation/","title":"Multinode GPU Fragmentation","text":"<p>This alert identifies GPU jobs that are using too many nodes or too few GPUs per node.</p> <p>Consider a cluster with 4 GPUs per node. A user can run a job with 8 GPUs by either (1) allocating 4 GPUs on 2 nodes or (2) allocating 1 GPU on 8 nodes. The former is in general strongly preferred. This alert catches jobs doing the latter, i.e., multinode jobs that allocate less than the number of available GPUs per node (e.g., 1 GPU on 8 nodes).</p> <p>Jobs with a GPU at 0% utilization are ignored since they will be captured by either Cancel 0% GPU Jobs or GPU-hours at 0% Utilization.</p>"},{"location":"alert/gpu_fragmentation/#configuration-file","title":"Configuration File","text":"<p>Below is an example entry for <code>config.yaml</code>:</p> <pre><code>multinode-gpu-fragmentation-1:\n  cluster: della\n  partitions:\n    - llm\n  gpus_per_node: 8  # count\n  min_run_time: 61  # minutes\n  email_file: \"multinode_gpu_fragmentation.txt\"\n  admin_emails:\n    - admin@institution.edu\n</code></pre> <p>The available settings are listed below:</p> <ul> <li> <p><code>cluster</code>: Specify the cluster name as it appears in the Slurm database.</p> </li> <li> <p><code>partitions</code>: Specify one or more Slurm partitions. Use <code>\"*\"</code> to include all partitions (i.e., <code>partitions: [\"*\"]</code>).</p> </li> <li> <p><code>gpus_per_node</code>: The number of GPUs per node.</p> </li> <li> <p><code>email_file</code>: The text file to be used as the email message to users.</p> </li> <li> <p><code>min_run_time</code>: (Optional) The number of minutes that a job must have ran to be considered. This can be used to exclude test jobs and experimental jobs. Default: 0</p> </li> <li> <p><code>include_running_jobs</code>: (Optional) If <code>True</code> then jobs in a state of <code>RUNNING</code> will be included in the calculation. The Prometheus server must be queried for each running job, which can be an expensive operation. Default: False</p> </li> <li> <p><code>nodelist</code>: (Optional) Only apply this alert to jobs that ran on the specified nodes. See example.</p> </li> <li> <p><code>excluded_qos</code>: (Optional) List of QOSes to exclude from this alert.</p> </li> <li> <p><code>excluded_partitions</code>: (Optional) List of partitions to exclude from this alert. This is useful when <code>partitions: [\"*\"]</code> is used.</p> </li> <li> <p><code>excluded_users</code>: (Optional) List of usernames to exclude from receiving emails.</p> </li> <li> <p><code>admin_emails</code>: (Optional) List of administrator email addresses that should receive copies of the emails that are sent to users.</p> </li> <li> <p><code>email_subject</code>: (Optional) Subject of the email message to users.</p> </li> <li> <p><code>report_title</code>: (Optional) Title of the report to system administrators.</p> </li> </ul>"},{"location":"alert/gpu_fragmentation/#report-for-system-administrators","title":"Report for System Administrators","text":"<p>Below is an example report for system administrators:</p> <pre><code>$ job_defense_shield --multinode-gpu-fragmentation\n\n             Multinode GPU Jobs with Fragmentation              \n----------------------------------------------------------------\n JobID     User   GPUs  Nodes GPUs-per-Node Hours GPU-Eff Emails\n----------------------------------------------------------------\n62666282  u42994   8     2          4          7    98%    0   \n62666283  u42994   8     2          4          6    98%    0   \n62666284  u42994   8     2          4          6    99%    0   \n62666285  u42994   8     2          4          7    98%    0   \n62666286  u42994   8     2          4          6    98%    0   \n62666287  u42994   8     2          4          6    98%    0   \n62666288  u42994   8     2          4          7    98%    0   \n62666289  u42994   8     2          4          7    99%    0   \n62666290  u42994   8     2          4          7    98%    0   \n62666291  u42994   8     2          4          7    98%    0   \n62816261  u18375   4     2          2        1.1    10%    1 (5)   \n----------------------------------------------------------------\n   Cluster: della\nPartitions: llm\n     Start: Mon Mar 12, 2025 at 11:18 AM\n       End: Wed Mar 19, 2025 at 11:18 AM\n</code></pre>"},{"location":"alert/gpu_fragmentation/#email-message-to-users","title":"Email Message to Users","text":"<p>Below is an example email message to a user (see <code>email/multinode_gpu_fragmentation.txt</code>):</p> <pre><code>Hello Alan (u42994),\n\nBelow are jobs that ran on Della in the past 7 days that used 1 GPU per node\nover multiple nodes:\n\n     JobID     User   GPUs  Nodes GPUs-per-Node  Hours State GPU-eff\n    60969293 aturing   4     2          2          24    TO     0%  \n\nThe GPU nodes on Della have 8 GPUs per node. For future jobs, please try to\nuse as few nodes as possible by allocating more GPUs per node. This is done\nby modifying the --gres Slurm directive.\n\nReplying to this automated email will open a support ticket with Research\nComputing.\n</code></pre>"},{"location":"alert/gpu_fragmentation/#placeholders","title":"Placeholders","text":"<p>The following placeholders can be used in the email file:</p> <ul> <li><code>&lt;GREETING&gt;</code>: Greeting generated by <code>greeting_method</code>.</li> <li><code>&lt;CLUSTER&gt;</code>: Name of the cluster specified for the alert.</li> <li><code>&lt;PARTITIONS&gt;</code>: A comma-separated list of partitions used by the user.</li> <li><code>&lt;GPUS-PER-NODE&gt;</code>: Number of GPUs per node (i.e., <code>gpus_per_node</code>).</li> <li><code>&lt;TABLE&gt;</code>: Table of job data.</li> <li><code>&lt;DAYS&gt;</code>: Number of days in the time window (default is 7).</li> </ul>"},{"location":"alert/gpu_fragmentation/#usage","title":"Usage","text":"<p>Generate a report for system administrators:</p> <pre><code>$ job_defense_shield --multinode-gpu-fragmentation\n</code></pre> <p>Send emails to the offending users:</p> <pre><code>$ job_defense_shield --multinode-gpu-fragmentation --email\n</code></pre> <p>See which users have received emails and when:</p> <pre><code>$ job_defense_shield --multinode-gpu-fragmentation --check\n</code></pre>"},{"location":"alert/gpu_fragmentation/#cron","title":"cron","text":"<p>Below is an example entry for <code>crontab</code>:</p> <pre><code>0 9 * * 1-5 /path/to/job_defense_shield --multinode-gpu-fragmentation --email -M della -r llm &gt; /path/to/log/gpu_fragmentation.log 2&gt;&amp;1\n</code></pre>"},{"location":"alert/gpu_model_too_powerful/","title":"GPU Model Too Powerful","text":"<p>This alert identifies jobs that ran on GPUs that were more powerful than necessary. For example, it can find jobs that ran on an NVIDIA B200 GPU but could have used a less powerful GPU (e.g., A100 or MIG). It can also find jobs that could have ran on GPUs with less memory. Jobs can be identified based on GPU utilization, CPU/GPU memory usage, and the number of allocated CPU-cores.</p>"},{"location":"alert/gpu_model_too_powerful/#configuration-file","title":"Configuration File","text":"<p>Here is an example entry for <code>config.yaml</code>:</p> <pre><code>gpu-model-too-powerful:\n  clusters: della\n  partitions:\n    - gpu\n  min_run_time:          61  # minutes\n  gpu_util_threshold:    20  # percent\n  gpu_mem_usage_max:     10  # GB\n  num_cores_per_gpu:     12  # count\n  cpu_mem_usage_per_gpu: 32  # GB\n  gpu_hours_threshold:   24  # gpu-hours\n  gpu_util_target:       50  # percent\n  email_file: \"gpu_model_too_powerful.txt\"\n  admin_emails:\n    - admin@institution.edu\n</code></pre> <p>The available settings are listed below:</p> <ul> <li> <p><code>cluster</code>: Specify the cluster name as it appears in the Slurm database.</p> </li> <li> <p><code>partitions</code>: Specify one or more Slurm partitions. Use <code>\"*\"</code> to include all partitions (i.e., <code>partitions: [\"*\"]</code>).</p> </li> <li> <p><code>gpu_util_threshold</code>: Jobs with a mean GPU utilization of less than or equal to this value will be included.</p> </li> <li> <p><code>email_file</code>: The text file to be used for the email message.</p> </li> <li> <p><code>num_cores_per_gpu</code>: (Optional) This quantity is the total number of CPU-cores divided by the number of allocated GPUs. Jobs with a number of cores per GPU of less than or equal to <code>num_cores_per_gpu</code> will be selected.</p> </li> <li> <p><code>gpu_mem_usage_max</code>: (Optional) Threshold for GPU memory usage in units of GB. Jobs with a GPU memory usage of less than or equal to <code>gpu_mem_usage_max</code> will be selected. For multi-GPU jobs, the maximum of the individual GPU memory usage values is used.</p> </li> <li> <p><code>cpu_mem_usage_per_gpu</code>: (Optional) Threshold for CPU memory usage per GPU in units of GB. Jobs with a CPU memory usage per GPU of less than or equal to <code>cpu_mem_usage_per_gpu</code> will be selected. For multi-GPU jobs, this is calculated as total CPU memory usage of the job divided by the total number of allocated GPUs.</p> </li> <li> <p><code>num_gpus</code>: (Optional) Jobs with a number of allocated GPUs of less than or equal to <code>num_gpus</code> will be selected.</p> </li> <li> <p><code>gpu_hours_threshold</code>: (Optional) Minimum number of GPU-hours (summed over the jobs) for the user to be included. This setting makes it possible to ignore users that are not consuming many resources. Default: 0</p> </li> <li> <p><code>gpu_util_target</code>: (Optional) The minimum acceptable GPU utilization. This must be specified for the <code>&lt;TARGET&gt;</code> placeholder to be available. Default: 50</p> </li> <li> <p><code>min_run_time</code>: (Optional) The minimum run time of a job for it to be considered. Jobs that did not run longer than this limit will be ignored. Default: 0</p> </li> <li> <p><code>include_running_jobs</code>: (Optional) If <code>True</code> then jobs in a state of <code>RUNNING</code> will be included in the calculation. The Prometheus server must be queried for each running job, which can be an expensive operation. Default: False</p> </li> <li> <p><code>nodelist</code>: (Optional) Only apply this alert to jobs that ran on the specified nodes. See example.</p> </li> <li> <p><code>excluded_qos</code>: (Optional) List of QOSes to exclude from this alert.</p> </li> <li> <p><code>excluded_partitions</code>: (Optional) List of partitions to exclude from this alert. This is useful when <code>partitions: [\"*\"]</code> is used.</p> </li> <li> <p><code>excluded_users</code>: (Optional) List of users to exclude from receiving emails.</p> </li> <li> <p><code>admin_emails</code>: (Optional) List of administrator email addresses that should receive copies of the emails that are sent to users.</p> </li> <li> <p><code>email_subject</code>: (Optional) Subject of the email message to users.</p> </li> <li> <p><code>report_title</code>: (Optional) Title of the report to system administrators.</p> </li> </ul> <p>Nodelist</p> <p>Be aware that a <code>nodelist</code> can be specified. This makes it possible to isolate jobs that ran on certain nodes within a partition.</p> <p>0% GPU Utilization</p> <p>Jobs with 0% GPU utilization are ignored. To capture these jobs, use a different alert such as GPU-Hours at 0% Utilization.</p>"},{"location":"alert/gpu_model_too_powerful/#report-for-system-administrators","title":"Report for System Administrators","text":"<pre><code>$ job_defense_shield --gpu-model-too-powerful\n\n                       GPU Model Too Powerful                       \n------------------------------------------------------------\n  User   GPU-Hours  Jobs            JobID             Emails\n------------------------------------------------------------\n u23157     321      5    2567707,62567708,62567709+   0   \n u55404     108      5   62520246,62520247,62861050+   1 (2)\n u89790      55      2            62560705,62669923    0   \n------------------------------------------------------------\n   Cluster: della\nPartitions: gpu\n     Start: Tue Mar 11, 2025 at 10:50 PM\n       End: Tue Mar 18, 2025 at 10:50 PM\n</code></pre>"},{"location":"alert/gpu_model_too_powerful/#email-message-to-users","title":"Email Message to Users","text":"<p>Below is an example email message (see <code>email/gpu_model_too_powerful.txt</code>):</p> <pre><code>Hello Alan (u23157),\n\nBelow are jobs that ran on an A100 GPU on Della in the past 7 days:\n\n   JobID   Cores  GPUs GPU-Util GPU-Mem-Used-Max CPU-Mem-Used/GPU  Hours\n  65698718   4     1      8%          4 GB             8 GB         120 \n  65698719   4     1     14%          1 GB             3 GB          30 \n  65698720   4     1      9%          2 GB             8 GB          90 \n\nGPU-Mem-Used-Max is the maximum GPU memory usage of the individual allocated\nGPUs while CPU-Mem-Used/GPU is the total CPU memory usage of the job divided by\nthe number of allocated GPUs.\n\nThe jobs above have (1) a low GPU utilization, (2) use less than 10 GB of GPU\nmemory, (3) use less than 32 GB of CPU memory, and (4) use 4 CPU-cores or less.\nSuch jobs could be run on the MIG GPUs. A MIG GPU has 1/7th the performance and\nmemory of an A100. To run on a MIG GPU, add the \"partition\" directive to your\nSlurm script:\n\n  #SBATCH --gres=gpu:1\n  #SBATCH --partition=mig\n\nFor interactive sessions use, for example:\n\n  $ salloc --nodes=1 --ntasks=&lt;N&gt; --time=1:00:00 --gres=gpu:1 --partition=mig\n\nReplying to this automated email will open a support ticket with Research\nComputing.\n</code></pre>"},{"location":"alert/gpu_model_too_powerful/#placeholders","title":"Placeholders","text":"<p>The following placeholders can be used in the email file:</p> <ul> <li><code>&lt;GREETING&gt;</code>: The greeting generated by <code>greeting-method</code>.</li> <li><code>&lt;CLUSTER&gt;</code>: The cluster specified for the alert.</li> <li><code>&lt;PARTITIONS&gt;</code>: A comma-separated list of partitions used by the user.</li> <li><code>&lt;DAYS&gt;</code>: Number of days in the time window (default is 7).</li> <li><code>&lt;TARGET&gt;</code>: The minimum acceptable GPU utilization (i.e., <code>gpu_util_target</code>).</li> <li><code>&lt;GPU-UTIL&gt;</code>: Threshold value for GPU utilization (i.e., <code>gpu_util_threshold</code>). </li> <li><code>&lt;CORES-PER-GPU&gt;</code>: Number of CPU-cores per GPU (i.e., <code>num_cores_per_gpu</code>).</li> <li><code>&lt;GPU-MEM&gt;</code>: Maximum GPU memory usage (i.e., <code>gpu_mem_usage_max</code>).</li> <li><code>&lt;CPU-MEM&gt;</code>: CPU memory usage per GPU (i.e., <code>cpu_mem_usage_per_gpu</code>).</li> <li><code>&lt;NUM-GPUS&gt;</code>: Threshold value for the number of allocated GPUs per job (i.e., <code>num_gpus</code>).</li> <li><code>&lt;NUM-JOBS&gt;</code>: Number of jobs that are using GPUs that are too powerful.</li> <li><code>&lt;TABLE&gt;</code>: Table of job data.</li> <li><code>&lt;JOBSTATS&gt;</code>: The <code>jobstats</code> command for the first job of the user.</li> </ul>"},{"location":"alert/gpu_model_too_powerful/#usage","title":"Usage","text":"<p>Generate a report of the users that are using GPUs that are more powerful than necessary:</p> <pre><code>$ job_defense_shield --gpu-model-too-powerful\n</code></pre> <p>Send emails to the offending users:</p> <pre><code>$ job_defense_shield --gpu-model-too-powerful --email\n</code></pre> <p>See which users have received emails and when:</p> <pre><code>$ job_defense_shield --gpu-model-too-powerful --check\n</code></pre>"},{"location":"alert/gpu_model_too_powerful/#cron","title":"cron","text":"<p>Below is an example entry for <code>crontab</code>:</p> <pre><code>0 9 * * * /path/to/job_defense_shield --gpu-model-too-powerful --email &gt; /path/to/log/gpu_model_too_powerful.log 2&gt;&amp;1\n</code></pre>"},{"location":"alert/low_cpu_util/","title":"Low CPU Utilization","text":"<p>This alert identifies users with low CPU efficiency.</p>"},{"location":"alert/low_cpu_util/#configuration-file","title":"Configuration File","text":"<p>Below is an example entry for <code>config.yaml</code>:</p> <pre><code>low-cpu-efficiency-1:\n  cluster: della\n  partitions:\n    - cpu\n  eff_thres_pct:         60  # percent\n  absolute_thres_hours: 100  # cpu-hours\n  eff_target_pct:        90  # percent\n  num_top_users:         15  # count\n  email_file: \"low_cpu_efficiency.txt\"\n  admin_emails:\n    - admin@institution.edu\n</code></pre> <p>The parameters are explained below:</p> <ul> <li> <p><code>cluster</code>: Specify the cluster name as it appears in the Slurm database. One cluster name per alert.</p> </li> <li> <p><code>partitions</code>: Specify one or more Slurm partitions. Use <code>\"*\"</code> to include all partitions (i.e., <code>partitions: [\"*\"]</code>).</p> </li> <li> <p><code>eff_thres_pct</code>: Efficiency threshold percentage. Users with a <code>eff_thres_pct</code> os less than or equal to this value will receive an email. plus more</p> </li> <li> <p><code>absolute_thres_hours</code>: A user must have used more than this number of CPU-hours to be considered to receive an email.</p> </li> <li> <p><code>eff_target_pct</code>: The target value for CPU utilization that users should strive for. It is only used in emails. This value can be referenced as the tag <code>&lt;TARGET&gt;</code> in email messages (see <code>low_cpu_efficiency.txt</code>).</p> </li> <li> <p><code>email_file</code>: The text file to be used as the email message to users.</p> </li> <li> <p><code>num_top_users</code>: (Optional) After sorting all users by CPU-hours, only consider this number of users for all remaining calculations and emails. This is used to limit the number of users that receive emails and appear in reports. Default: 15</p> </li> <li> <p><code>show_all_users</code>: (Optional) Flag to show all of the top users in the report instead of only the top users with low efficiency. Default: False</p> </li> <li> <p><code>min_run_time</code>: (Optional) Minimum run time of a job in units of minutes. If <code>min_run_time: 61</code> then jobs that ran for an hour or less are ignored. Default: 0</p> </li> <li> <p><code>proportion_thres_pct</code>: (Optional) Proportional threshold percentage. A user must being using at least this proportion of the total CPU-hours (as a percentage) in order to be sent an email. For example, setting this to 2 will excluded all users that are using less than 2% of the total CPU-hours. Default: 0</p> </li> <li> <p><code>nodelist</code>: (Optional) Only apply this alert to jobs that ran on the specified nodes. See example.</p> </li> <li> <p><code>excluded_qos</code>: (Optional) List of QOSes to exclude from this alert.</p> </li> <li> <p><code>excluded_partitions</code>: (Optional) List of partitions to exclude from this alert. This is useful when <code>partitions: [\"*\"]</code> is used.</p> </li> <li> <p><code>excluded_users</code>: (Optional) List of users to exclude from receiving emails.</p> </li> <li> <p><code>admin_emails</code>: (Optional) List of administrator email addresses that should receive copies of the emails that are sent to users.</p> </li> <li> <p><code>email_subject</code>: (Optional) Subject of the email message to users.</p> </li> <li> <p><code>report_title</code>: (Optional) Title of the report to system administrators.</p> </li> </ul> <p>How is CPU efficiency calculated?</p> <p>The CPU efficiency is weighted by the number of CPU-cores per job. Jobs with 0% utilization on a node are ignored since they are captured by another alert.</p>"},{"location":"alert/low_cpu_util/#report-for-system-administrators","title":"Report for System Administrators","text":"<p>Below is an example report:</p> <pre><code>$ job_defense_shield --low-cpu-efficiency\n\n                     Low CPU Efficiencies                                  \n-----------------------------------------------------------------\n User   CPU-Hours  Proportion(%)  CPU-Eff  Jobs  AvgCores  Emails\n-----------------------------------------------------------------\nu12345    16377         4           58%     998    15.8     0   \nu85632    12536         3           14%    1034    16.3     2 (6)   \nu39731    10227         2           50%    2477     2.0     0   \n-----------------------------------------------------------------\n   Cluster: della\nPartitions: cpu\n     Start: Wed Mar 12, 2025 at 02:05 PM\n       End: Wed Mar 19, 2025 at 02:05 PM\n</code></pre>"},{"location":"alert/low_cpu_util/#email-message-to-users","title":"Email Message to Users","text":"<p>Below is an example email (see <code>email/low_cpu_efficiency.txt</code>):</p> <pre><code>Hello Alan (u12345),\n\nOver the last 7 days you have used the 3rd most CPU-hours on della (cpu) but\nyour mean CPU efficiency is only 23%:\n\n     User  Partition(s)  Jobs  CPU-Hours CPU-Rank Efficiency AvgCores\n    u12345     cpu        33     29062     3/250      23%       8    \n\nA good target value for \"Efficiency\" is 90% and above. Please investigate the reason\nfor the low efficiency. Common reasons for low CPU efficiency are discussed here:\n\n    https://your-institution.edu/KB/cpu-utilization\n\nReplying to this automated email will open a support ticket with Research\nComputing.\n</code></pre>"},{"location":"alert/low_cpu_util/#placeholders","title":"Placeholders","text":"<p>The following placeholders can be used in the email file:</p> <ul> <li><code>&lt;GREETING&gt;</code>: Greeting generated by <code>greeting-method</code>.</li> <li><code>&lt;CLUSTER&gt;</code>: The name of the cluster.</li> <li><code>&lt;PARTITIONS&gt;</code>: A comma-separated list of partitions used by the user.</li> <li><code>&lt;DAYS&gt;</code>: Number of days in the time window (default is 7 days).</li> <li><code>&lt;EFFICIENCY&gt;</code>: Mean CPU efficiency of the user (e.g., 23%).</li> <li><code>&lt;TARGET&gt;</code>: Target value for the mean CPU efficiency.</li> <li><code>&lt;TABLE&gt;</code>: A table of jobs for the user.</li> <li><code>&lt;JOBSTATS&gt;</code>: The <code>jobstats</code> command for the first job of the user.</li> </ul>"},{"location":"alert/low_cpu_util/#usage","title":"Usage","text":"<p>Generate a report for system administrators:</p> <pre><code>$ job_defense_shield --low-cpu-efficiency\n</code></pre> <p>Send emails to the offending users:</p> <pre><code>$ job_defense_shield --low-cpu-efficiency --email\n</code></pre> <p>See which users have received emails and when:</p> <pre><code>$ job_defense_shield --low-cpu-efficiency --check\n</code></pre>"},{"location":"alert/low_cpu_util/#cron","title":"cron","text":"<p>Below is an example <code>crontab</code> entry:</p> <pre><code>0 9 * * * /path/to/job_defense_shield --low-cpu-efficiency --email &gt; /path/to/log/low_cpu_efficiency.log 2&gt;&amp;1\n</code></pre>"},{"location":"alert/low_gpu_util/","title":"Low GPU Utilization","text":"<p>This alert identifies users with low GPU efficiency.</p>"},{"location":"alert/low_gpu_util/#configuration-file","title":"Configuration File","text":"<p>Below is an example entry for <code>config.yaml</code>:</p> <pre><code>low-gpu-efficiency-1:\n  cluster: della\n  partitions:\n    - gpu\n  min_run_time:         30  # minutes\n  eff_thres_pct:        25  # percent\n  eff_target_pct:       50  # percent\n  absolute_thres_hours: 50  # gpu-hours\n  proportion_thres_pct:  2  # percent\n  num_top_users:        15  # count\n  email_file: \"low_gpu_efficiency.txt\"\n  admin_emails:\n    - admin@institution.edu\n</code></pre> <p>The available settings are listed below:</p> <ul> <li> <p><code>cluster</code>: Specify the cluster name as it appears in the Slurm database.</p> </li> <li> <p><code>partitions</code>: Specify one or more Slurm partitions. Use <code>\"*\"</code> to include all partitions (i.e., <code>partitions: [\"*\"]</code>). The number of GPU-hours is summed over all of the partitions.</p> </li> <li> <p><code>eff_thres_pct</code>: Efficiency threshold percentage. Users with a mean GPU efficiency of less than or equal to this value will be considered to receive an email.</p> </li> <li> <p><code>eff_target_pct</code>: The minimum acceptable mean GPU utilization for a user. This quantity is not used in any calculations but it can be referenced using <code>&lt;TARGET&gt;</code> in the email file.</p> </li> <li> <p><code>absolute_thres_hours</code>: Absolute threshold hours. A user must have consumed more than this number of GPU-hours to receive an email.</p> </li> <li> <p><code>email_file</code>: The text file to be used as the email message to users.</p> </li> <li> <p><code>num_top_users</code>: (Optional) After sorting all users by GPU-hours, only consider the top <code>num_top_users</code> for all remaining calculations and emails. This is used to limit the number of users that receive emails and appear in reports. Default: 15</p> </li> <li> <p><code>show_all_users</code>: (Optional) Flag to show all of the top users in the report instead of only the top users with low efficiency. Default: False</p> </li> <li> <p><code>min_run_time</code>: (Optional) The number of minutes that a job must have ran to be considered. Default: 0</p> </li> <li> <p><code>proportion_thres_pct</code>: (Optional) Proportion threshold percentage. A user must have consumed at least this proportion of the total GPU-hours (as a percentage) in order to be sent an email. For example, a value of 2 will exclude all users that are using less than 2% of the total GPU-hours. Default: 0</p> </li> <li> <p><code>gpu_mem_eff_pct</code>: (Optional) Threshold for GPU memory efficiency as a percentage. Jobs with a GPU memory efficiency of greater than this value will be ignored. A job that uses 60 GB of an 80 GB GPU has a memory efficiency of 75%. For multi-GPU jobs, the mean GPU memory efficiency is used. This setting makes it possible to ignore jobs with low GPU utilization but high GPU memory usage. Default: 100</p> </li> <li> <p><code>nodelist</code>: (Optional) Only apply this alert to jobs that ran on the specified nodes. See example.</p> </li> <li> <p><code>excluded_qos</code>: (Optional) List of QOSes to exclude from this alert.</p> </li> <li> <p><code>excluded_partitions</code>: (Optional) List of partitions to exclude from this alert. This is useful when <code>partitions: [\"*\"]</code> is used.</p> </li> <li> <p><code>excluded_users</code>: (Optional) List of usernames to exclude from receiving emails.</p> </li> <li> <p><code>admin_emails</code>: (Optional) List of administrator email addresses that should receive copies of the emails that are sent to users.</p> </li> <li> <p><code>email_subject</code>: (Optional) Subject of the email message to users.</p> </li> <li> <p><code>report_title</code>: (Optional) Title of the report to system administrators.</p> </li> </ul>"},{"location":"alert/low_gpu_util/#report-for-system-administrators","title":"Report for System Administrators","text":"<p>Here is an example of the report:</p> <pre><code>$ job_defense_shield --low-gpu-efficiency\n\n                    Low GPU Efficiencies                                      \n------------------------------------------------------------\n User   GPU-Hours  Proportion(%)  GPU-Eff(%)  Jobs  AvgCores\n------------------------------------------------------------\nu76174    3791         20             19       58     1.2  \nu64732    3201         17             15       43     1.0 \nu13301    2281         12              9       35     8.0\n------------------------------------------------------------\n   Cluster: della\nPartitions: gpu\n     Start: Wed Mar 12, 2025 at 09:56 AM\n       End: Wed Mar 19, 2025 at 09:56 AM\n</code></pre>"},{"location":"alert/low_gpu_util/#email-message-to-users","title":"Email Message to Users","text":"<p>Below is an example email (see <code>email/low_gpu_efficiencies.txt</code>):</p> <pre><code>Hello Alan (u12345),\n\nOver the last 7 days you have used the 3rd most GPU-hours on della (pli-c) but\nyour mean GPU efficiency is only 29%:\n\n     User  Partition(s)  Jobs  GPU-hours GPU-rank Efficiency\n    jg9945    pli-c       65     2345      3/53      29%    \n\nA good target value for \"Efficiency\" is 75% and above. Please investigate the\nreason for the low efficiency.\n\nReplying to this automated email will open a support ticket with Research\nComputing.\n</code></pre>"},{"location":"alert/low_gpu_util/#placeholders","title":"Placeholders","text":"<p>The following placeholders can be used in the email file:</p> <ul> <li><code>&lt;GREETING&gt;</code>: The greeting generated by <code>greeting-method</code>.</li> <li><code>&lt;CLUSTER&gt;</code>: The name of the cluster as defined in <code>config.yaml</code>.</li> <li><code>&lt;PARTITIONS&gt;</code>: A comma-separated list of the partitions used by the user.</li> <li><code>&lt;EFFICIENCY&gt;</code>: The mean GPU efficiency of the user.</li> <li><code>&lt;RANK&gt;</code>: The GPU-rank of the user as determined by number of allocated GPU-hours.</li> <li><code>&lt;GPU-MEM-PCT&gt;</code>: The value of <code>gpu_mem_eff_pct</code> from the alert settings.</li> <li><code>&lt;DAYS&gt;</code>: Number of days in the time window (default is 7).</li> <li><code>&lt;TARGET&gt;</code>: The value of <code>eff_target_pct</code> from the alert settings.</li> <li><code>&lt;TABLE&gt;</code>: A table of showing mean efficiency and other values for the user.</li> <li><code>&lt;JOBSTATS&gt;</code>: The <code>jobstats</code> command for the first job of the user.</li> </ul>"},{"location":"alert/low_gpu_util/#usage","title":"Usage","text":"<p>Generate a report of the top users with low GPU efficiencies:</p> <pre><code>$ job_defense_shield --low-gpu-efficiency\n</code></pre> <p>Send emails to users with low GPU efficiencies over the past 7 days:</p> <pre><code>$ job_defense_shield --low-gpu-efficiency --email\n</code></pre> <p>See which users have received emails and when:</p> <pre><code>$ job_defense_shield --low-gpu-efficiency --check\n</code></pre>"},{"location":"alert/low_gpu_util/#cron","title":"cron","text":"<p>Below is an example entry for <code>crontab</code>:</p> <pre><code>0 9 * * * /path/to/job_defense_shield --low-gpu-efficiency --email &gt; /path/to/log/low_gpu_efficiency.log 2&gt;&amp;1\n</code></pre>"},{"location":"alert/new_alert/","title":"New Alerts","text":"<p>Have a good idea for a new alert? Let us know what you are thinking by posting a GitHub issue.</p>"},{"location":"alert/serial_allocating_multiple/","title":"Serial Jobs Allocating Multiple CPU-Cores","text":"<p>This alert identifies users that are apparently running serial codes while allocating multiple CPU-cores. A serial code can only use 1 CPU-core so the additional allocated cores are wasted.</p>"},{"location":"alert/serial_allocating_multiple/#configuration-file","title":"Configuration File","text":"<p>Below is an example entry for <code>config.yaml</code>:</p> <pre><code>serial-allocating-multiple-1:\n  cluster: della\n  partitions:\n    - cpu\n  min_run_time:         61  # minutes\n  cpu_hours_threshold: 100  # cpu-hours\n  lower_ratio:        0.85  # [0.0, 1.0]\n  num_top_users:         5  # count\n  email_file: \"serial_allocating_multiple.txt\"\n  admin_emails:\n    - admin@institution.edu\n</code></pre> <p>The parameters are explained below:</p> <ul> <li> <p><code>cluster</code>: Specify the cluster name as it appears in the Slurm database.</p> </li> <li> <p><code>partitions</code>: Specify one or more Slurm partitions. Use <code>\"*\"</code> to include all partitions (i.e., <code>partitions: [\"*\"]</code>).</p> </li> <li> <p><code>cpu_hours_threshold</code>: A user must have apparently wasted this number of CPU-hours to be considered.</p> </li> <li> <p><code>lower_ratio</code>: This alert works by comparing the CPU efficiency of the job to 100% divided by the number of allocated CPU-cores. If the ratio of these two numbers of greater than or equal to <code>lower_ratio</code> then the job is assumed to be a serial code wasting CPU-cores. For instance, if the CPU efficieny of a job that allocates 8 CPU-cores is 12.4% and <code>lower_ratio</code> is 0.85 then this job would be included since 12.4% / (100% / 8) is greater than 0.85. Default: 0.85</p> </li> <li> <p><code>email_file</code>: The text file to be used for the email message to users.</p> </li> <li> <p><code>min_run_time</code>: (Optional) Minimum run time of a job in units of minutes. If <code>min_run_time: 61</code> then jobs that ran for an hour or less are ignored. Default: 0</p> </li> <li> <p><code>cores_per_node</code>: (Optional) Number of CPU-cores per node. If this is defined then the <code>&lt;NUM-NODES&gt;</code> placeholder will be available.</p> </li> <li> <p><code>num_top_users</code>: (Optional) Only consider up to this number of users after sorting by \"wasted CPU-hours\".</p> </li> <li> <p><code>ignore_job_arrays</code>: (Optional) Ignore jobs in a job array. Default: False</p> </li> <li> <p><code>include_running_jobs</code>: (Optional) If <code>True</code> then jobs in a state of <code>RUNNING</code> will be included in the calculation. The Prometheus server must be queried for each running job, which can be an expensive operation. Default: False</p> </li> <li> <p><code>nodelist</code>: (Optional) Only apply this alert to jobs that ran on the specified nodes. See example.</p> </li> <li> <p><code>excluded_qos</code>: (Optional) List of QOSes to exclude from this alert.</p> </li> <li> <p><code>excluded_partitions</code>: (Optional) List of partitions to exclude from this alert. This is useful when <code>partitions: [\"*\"]</code> is used.</p> </li> <li> <p><code>excluded_users</code>: (Optional) List of users to exclude from receiving emails.</p> </li> <li> <p><code>admin_emails</code>: (Optional) List of administrator email addresses that should receive copies of the emails that are sent to users.</p> </li> <li> <p><code>email_subject</code>: (Optional) Subject of the email message to users.</p> </li> <li> <p><code>report_title</code>: (Optional) Title of the report to system administrators.</p> </li> </ul>"},{"location":"alert/serial_allocating_multiple/#report-for-system-administrators","title":"Report for System Administrators","text":"<p>Below is an example report:</p> <pre><code>$ job_defense_shield --serial-allocating-multiple\n\n               Serial Jobs Allocating Multiple CPU-Cores                        \n------------------------------------------------------------------------\n    User   CPU-Hours-Wasted AvgCores  Jobs        JobID           Emails\n------------------------------------------------------------------------\n1  u74805       10248           8      96   62829596,62834509+    2 (3)\n2  u31448         906          30       2   62887370,62887407     0     \n3  u93676         783           3      20   62904738,62904739+    1 (10)\n4  u16959         325          15       6   62896573,62912383+    4 (17)\n5  u36725         164          12      11   62814801,62814802+    1 (10)\n------------------------------------------------------------------------\n   Cluster: della\nPartitions: cpu\n     Start: Sun Mar 09, 2025 at 10:21 PM\n       End: Sun Mar 16, 2025 at 10:21 PM\n</code></pre>"},{"location":"alert/serial_allocating_multiple/#email-message-to-users","title":"Email Message to Users","text":"<p>Below is an example email (see <code>email/serial_allocating_multiple.txt</code>):</p> <pre><code>Hello Alan (u12345),\n\nBelow are your jobs that ran on della (cpu) in the past 7 days:\n\n      JobID   Partition  CPU-Cores  CPU-Util  100%/CPU-cores  Hours\n    62599800     cpu         4       24.8%        25.0%        48  \n    62599800     cpu         4       24.9%        25.0%        48  \n    62695534     cpu        12        8.3%         8.3%        24  \n    62719003     cpu        12        8.3%         8.3%        24  \n\nThe CPU utilization (CPU-Util) of each job above is approximately equal to\n100% divided by the number of allocated CPU-cores (100%/CPU-cores). This\nsuggests that you may be running a code that can only use 1 CPU-core. If this is\ntrue then allocating more than 1 CPU-core is wasteful. A good target value for\nCPU utilization is 90% and above.\n\nIf the code cannot run in parallel then please use the following Slurm\ndirectives:\n\n    #SBATCH --nodes=1\n    #SBATCH --ntasks=1\n    #SBATCH --cpus-per-task=1\n\nReplying to this automated email will open a support ticket with Research\nComputing.\n</code></pre>"},{"location":"alert/serial_allocating_multiple/#placeholders","title":"Placeholders","text":"<p>The following placeholders can be used in the email file:</p> <ul> <li><code>&lt;GREETING&gt;</code>: The greeting generated by <code>greeting-method</code>.</li> <li><code>&lt;CLUSTER&gt;</code>: The cluster specified for the alert.</li> <li><code>&lt;PARTITIONS&gt;</code>: A comma-separated list of the partitions used by the user.</li> <li><code>&lt;CASE&gt;</code>: The rank of the user by CPU-hours.</li> <li><code>&lt;DAYS&gt;</code>: Number of days in the time window (default is 7).</li> <li><code>&lt;CPU-HOURS&gt;</code>: Total number of CPU-hours wasted if the jobs were in fact running serial codes. This quantity is the elapsed time of the job multiplied by the number of allocated core minus one (summed over all jobs).</li> <li><code>&lt;NUM-JOBS&gt;</code>: Total number of jobs with at least one idle GPU.</li> <li><code>&lt;TABLE&gt;</code>: Table of job data.</li> <li><code>&lt;JOBSTATS&gt;</code>: The <code>jobstats</code> command for the first job of the user.</li> </ul> <p>If <code>cores_per_node</code> is defined in the alert then one additional placeholder is available:</p> <ul> <li><code>&lt;NUM-NODES&gt;</code>: Number of nodes wasted. This is calculated as the wasted CPU-hours divided by the product of the number of cores per node and the number of hours in the time window. One strategy is to set <code>cpu_hours_threshold</code> large enough so that <code>&lt;NUM-NODES&gt;</code> is always greater than or equal to 1. Note that the <code>round</code> function is applied when computing this quantity.</li> </ul>"},{"location":"alert/serial_allocating_multiple/#usage","title":"Usage","text":"<p>Generate a report for system administrators:</p> <pre><code>$ job_defense_shield --serial-allocating-multiple\n</code></pre> <p>Send emails to the offending users:</p> <pre><code>$ job_defense_shield --serial-allocating-multiple --email\n</code></pre> <p>See which users have received emails and when:</p> <pre><code>$ job_defense_shield --serial-allocating-multiple --check\n</code></pre>"},{"location":"alert/serial_allocating_multiple/#cron","title":"cron","text":"<p>Below is an example <code>crontab</code> entry:</p> <pre><code>0 9 * * 1-5 /path/to/job_defense_shield --serial-allocating-multiple --email &gt; /path/to/log/serial_allocating_multiple.log 2&gt;&amp;1\n</code></pre>"},{"location":"alert/time_limits/","title":"Excessive Run Time Limits for CPU Jobs","text":"<p>This alert identifies users with excessive run time limits for CPU jobs (e.g., requesting 3 days but only needing 3 hours).</p>"},{"location":"alert/time_limits/#configuration-file","title":"Configuration File","text":"<p>Below is an example entry for <code>config.yaml</code>:</p> <pre><code>excessive-time-cpu-1:\n  cluster: della\n  partitions:\n    - cpu\n  min_run_time:             61  # minutes\n  absolute_thres_hours: 100000  # unused cpu-hours\n  overall_ratio_threshold: 0.2  # [0.0, 1.0]\n  mean_ratio_threshold:    0.2  # [0.0, 1.0]\n  median_ratio_threshold:  0.2  # [0.0, 1.0]\n  num_top_users:            10  # count\n  num_jobs_display:         10  # count\n  email_file: \"excessive_time.txt\"\n  admin_emails:\n    - admin@institution.edu\n</code></pre> <p>The parameters are explained below:</p> <ul> <li> <p><code>cluster</code>: Specify the cluster name as it appears in the Slurm database.</p> </li> <li> <p><code>partitions</code>: Specify one or more Slurm partitions. Use <code>\"*\"</code> to include all partitions (i.e., <code>partitions: [\"*\"]</code>).</p> </li> <li> <p><code>absolute_thres_hours</code>: Minimum number of unused CPU-hours for the user to be included.</p> </li> <li> <p><code>overall_ratio_threshold</code>: Total used CPU-hours divided by total allocated CPU-hours.</p> </li> <li> <p><code>email_file</code>: The text file to be used for the email message to users.</p> </li> <li> <p><code>mean_ratio_threshold</code>: (Optional) Mean of the per job ratio of used CPU-hours to allocated CPU-hours. Default: 1</p> </li> <li> <p><code>median_ratio_threshold</code>: (Optional) Same as above but for the median instead of the mean. Default: 1</p> </li> <li> <p><code>min_run_time</code>: (Optional) Minimum run time of a job in units of minutes. If <code>min_run_time: 61</code> then jobs that ran for an hour or less are ignored. Default: 0</p> </li> <li> <p><code>num_top_users</code>: (Optional) Only consider the number of users equal to this value after sorting by unused CPU-hours. Default: 10</p> </li> <li> <p><code>show_all_users</code>: (Optional) Flag to show all of the top users in the report instead of only the top users with low time efficiency. Default: False</p> </li> <li> <p><code>num_jobs_display</code>: (Optional) Number of jobs to display in the email message to users. Default: 10</p> </li> <li> <p><code>nodelist</code>: (Optional) Only apply this alert to jobs that ran on the specified nodes. See example.</p> </li> <li> <p><code>excluded_qos</code>: (Optional) List of QOSes to exclude from this alert.</p> </li> <li> <p><code>excluded_partitions</code>: (Optional) List of partitions to exclude from this alert. This is useful when <code>partitions: [\"*\"]</code> is used.</p> </li> <li> <p><code>excluded_users</code>: (Optional) List of usernames to exclude from the alert.</p> </li> <li> <p><code>admin_emails</code>: (Optional) List of administrator email addresses that should receive copies of the emails that are sent to users.</p> </li> <li> <p><code>email_subject</code>: (Optional) Subject of the email message to users.</p> </li> <li> <p><code>report_title</code>: (Optional) Title of the report to system administrators.</p> </li> </ul>"},{"location":"alert/time_limits/#report-for-system-administrators","title":"Report for System Administrators","text":"<p>Below is an example report:</p> <pre><code>$ job_defense_shield --excessive-time-cpu\n\n                         Excessive Time Limits                          \n-------------------------------------------------------------------------\n  User   CPU-Hours  CPU-Hours  Ratio  Ratio   Ratio CPU-Rank Jobs  Emails\n          (Unused)    (Used)  Overall  Mean  Median\n-------------------------------------------------------------------------\n  u18587   495341      13716    0.03   0.03   0.02     10     643   1 (4)\n  u81279   105666      80061    0.43   0.43   0.38      2      41   0\n  u45521   105509      21595    0.17   0.17   0.13      8     109   2 (1)\n  u73275    89469       8475    0.09   0.14   0.09     16      86   0\n  u43409    88689     125583    0.59   0.59   0.59      1     279   0\n-------------------------------------------------------------------------\n   Cluster: della\nPartitions: cpu\n     Start: Mon Mar 10, 2025 at 06:23 PM\n       End: Mon Mar 17, 2025 at 06:23 PM\n</code></pre> <p><code>CPU-Rank</code> is the rank of the user by used CPU-hours.</p>"},{"location":"alert/time_limits/#email-message-to-users","title":"Email Message to Users","text":"<p>Below is an example email (see <code>email/excessive_time.txt</code>):</p> <pre><code>Hello Alan (u18587),\n\nBelow are 5 of your 643 jobs that ran on della (cpu) in the past 7 days:\n\n     JobID   Time-Used Time-Allocated Percent-Used  CPU-Cores\n    62776009  01:36:11   2-00:00:00        3%          32    \n    62776014  01:32:11   2-00:00:00        3%          32    \n    62776016  01:22:41   2-00:00:00        3%          32    \n    62776019  01:17:48   2-00:00:00        3%          32    \n    62776020  01:29:20   2-00:00:00        3%          32    \n\nIt appears that you are requesting too much time for your jobs since you are\nonly using on average 3% of the allocated time (for the 643 jobs). This has\nresulted in 495341 CPU-hours that you scheduled but did not use (it was made\navailable to other jobs, however).\n\nPlease request less time by modifying the --time Slurm directive. This will\nlower your queue times and allow the Slurm job scheduler to work more\neffectively for all users.\n\nReplying to this automated email will open a support ticket with Research\nComputing.\n</code></pre>"},{"location":"alert/time_limits/#placeholders","title":"Placeholders","text":"<p>The following placeholders can be used in the email file:</p> <ul> <li><code>&lt;GREETING&gt;</code>: Greeting generated by <code>greeting-method</code>.</li> <li><code>&lt;CLUSTER&gt;</code>: Name of the cluster.</li> <li><code>&lt;PARTITIONS&gt;</code>: A comma-separated list of partitions used by the user.</li> <li><code>&lt;DAYS&gt;</code>: Number of days in the time window (default is 7).</li> <li><code>&lt;MODE-UPPER&gt;</code>: Equal to \"CPU\".</li> <li><code>&lt;AVERAGE&gt;</code>: Mean of the per-job CPU-hours used divided by CPU-hours allocated.</li> <li><code>&lt;NUM-JOBS&gt;</code>: Total number of jobs.</li> <li><code>&lt;NUM-JOBS-DISPLAY&gt;</code>: Number of jobs to list in the table.</li> <li><code>&lt;TABLE&gt;</code>: Table of job data.</li> <li><code>&lt;UNUSED-HOURS&gt;</code>: Total number of unused CPU-hours.</li> </ul>"},{"location":"alert/time_limits/#usage","title":"Usage","text":"<p>Generate a report for system administrators:</p> <pre><code>$ job_defense_shield --excessive-time-cpu\n</code></pre> <p>Send emails to the offending users:</p> <pre><code>$ job_defense_shield --excessive-time-cpu --email\n</code></pre> <p>See which users have received emails and when:</p> <pre><code>$ job_defense_shield --excessive-time-cpu --check\n</code></pre>"},{"location":"alert/time_limits/#cron","title":"cron","text":"<p>Below is an example <code>crontab</code> entry:</p> <pre><code>0 9 * * 1-5 /path/to/job_defense_shield --excessive-time-cpu --email &gt; /path/to/log/excessive_time_cpu.log 2&gt;&amp;1\n</code></pre>"},{"location":"alert/time_limits_gpu/","title":"Excessive Run Time Limits for GPU Jobs","text":"<p>This alert identifies users that are using excessive run time limits for GPU jobs (e.g., requesting 3 days but only needing 3 hours).</p>"},{"location":"alert/time_limits_gpu/#configuration-file","title":"Configuration File","text":"<p>Below is an example entry for <code>config.yaml</code>:</p> <pre><code>excessive-time-gpu-1:\n  cluster: della\n  partitions:\n    - gpu\n    - llm\n  min_run_time:             61  # minutes\n  absolute_thres_hours:   1000  # unused gpu-hours\n  overall_ratio_threshold: 0.2  # [0.0, 1.0]\n  mean_ratio_threshold:    0.2  # [0.0, 1.0]\n  median_ratio_threshold:  0.2  # [0.0, 1.0]\n  num_top_users:            10  # count\n  num_jobs_display:         10  # count\n  email_file: \"excessive_time.txt\"\n  admin_emails:\n    - admin@institution.edu\n</code></pre> <p>The available settings are listed below:</p> <ul> <li> <p><code>cluster</code>: Specify the cluster name as it appears in the Slurm database.</p> </li> <li> <p><code>partitions</code>: Specify one or more Slurm partitions. Use <code>\"*\"</code> to include all partitions (i.e., <code>partitions: [\"*\"]</code>).</p> </li> <li> <p><code>absolute_thres_hours</code>: Minimum number of unused GPU-hours for the user to be considered to receive an email.</p> </li> <li> <p><code>overall_ratio_threshold</code>: Total used GPU-hours divided by total allocated GPU-hours.</p> </li> <li> <p><code>email_file</code>: The text file to be used for the email message to users.</p> </li> <li> <p><code>mean_ratio_threshold</code>: (Optional) Mean of the per job ratio of used GPU-hours to allocated GPU-hours.</p> </li> <li> <p><code>median_ratio_threshold</code>: (Optional) Same as above but for the median instead of the mean.</p> </li> <li> <p><code>min_run_time</code>: (Optional) Minimum run time in minutes for a job to be included in the calculation. For example, if <code>min_run_time: 30</code> is used then jobs that ran for less than 30 minutes are ignored. Default: 0</p> </li> <li> <p><code>num_top_users</code>: (Optional) Only consider the number of users equal to <code>num_top_users</code> after sorting by unused GPU-hours. Default: 10</p> </li> <li> <p><code>show_all_users</code>: (Optional) Flag to show all of the top users in the report instead of only the top users with low time efficiency. Default: False</p> </li> <li> <p><code>num_jobs_display</code>: (Optional) Number of jobs to display in the email message to users. Default: 10</p> </li> <li> <p><code>nodelist</code>: (Optional) Only apply this alert to jobs that ran on the specified nodes. See example.</p> </li> <li> <p><code>excluded_qos</code>: (Optional) List of QOSes to exclude from this alert.</p> </li> <li> <p><code>excluded_partitions</code>: (Optional) List of partitions to exclude from this alert. This is useful when <code>partitions: [\"*\"]</code> is used.</p> </li> <li> <p><code>excluded_users</code>: (Optional) List of usernames to exclude from the alert.</p> </li> <li> <p><code>admin_emails</code>: (Optional) List of administrator email addresses that should receive copies of the emails that are sent to users.</p> </li> <li> <p><code>email_subject</code>: (Optional) Subject of the email message to users.</p> </li> <li> <p><code>report_title</code>: (Optional) Title of the report to system administrators.</p> </li> </ul>"},{"location":"alert/time_limits_gpu/#report-for-system-administrators","title":"Report for System Administrators","text":"<p>Below is an example report:</p> <pre><code>$ job_defense_shield --excessive-time-gpu\n\n                      Excessive Run Time Limits                        \n-----------------------------------------------------------------------\n User   GPU-Hours  GPU-Hours  Ratio  Ratio   Ratio GPU-Rank Jobs Emails\n         (Unused)    (Used)  Overall  Mean  Median                     \n-----------------------------------------------------------------------\nu14480    4088       184      0.04   0.04   0.04     15     60    0   \nu72284    2055       105      0.05   0.05   0.04     23     45    2 (3)   \n-----------------------------------------------------------------------\n   Cluster: della\nPartitions: gpu, llm\n     Start: Sat Mar 15, 2025 at 02:34 PM\n       End: Sat Mar 22, 2025 at 02:34 PM\n</code></pre>"},{"location":"alert/time_limits_gpu/#email-message-to-users","title":"Email Message to Users","text":"<p>Below is an example email message (see <code>email/excessive_time.txt</code>):</p> <pre><code>Hello Alan (u14480),\n\nBelow are 5 of your 60 jobs that ran on della (gpu) in the past 7 days:\n\n     JobID    Time-Used  Time-Allocated  Percent-Used  GPUs\n    62776009   01:36:11    2-00:00:00         3%        4 \n    62776014   01:32:11    2-00:00:00         3%        4\n    62776016   01:22:41    2-00:00:00         3%        4\n    62776019   01:17:48    2-00:00:00         3%        4\n    62776020   01:29:20    2-00:00:00         3%        4\n\nIt appears that you are requesting too much time for your jobs since you are\nonly using on average 3% of the allocated time (for the 60 jobs). This has\nresulted in 4341 GPU-hours that you scheduled but did not use (it was made\navailable to other jobs, however).\n\nPlease request less time by modifying the --time Slurm directive. This will\nlower your queue times and allow the Slurm job scheduler to work more\neffectively for all users.\n\nTime-Used is the time (wallclock) that the job needed. The total time allocated\nfor the job is Time-Allocated. The format is DD-HH:MM:SS where DD is days,\nHH is hours, MM is minutes and SS is seconds. Percent-Used is Time-Used\ndivided by Time-Allocated.\n\nReplying to this automated email will open a support ticket with Research\nComputing.\n</code></pre>"},{"location":"alert/time_limits_gpu/#placeholders","title":"Placeholders","text":"<p>The following placeholders can be used in the email file:</p> <ul> <li><code>&lt;GREETING&gt;</code>: Greeting generated by <code>greeting-method</code>.</li> <li><code>&lt;CLUSTER&gt;</code>: The cluster specified for the alert.</li> <li><code>&lt;PARTITIONS&gt;</code>: A comma-separated list of partitions used by the user.</li> <li><code>&lt;MODE-UPPER&gt;</code>: Equal to \"GPU\".</li> <li><code>&lt;CASE&gt;</code>: Helper text relating to jobs.</li> <li><code>&lt;AVERAGE&gt;</code>: Mean of per-job GPU-hours used divided by allocated.</li> <li><code>&lt;DAYS&gt;</code>: Number of days in the time window (default is 7).</li> <li><code>&lt;NUM-JOBS&gt;</code>: Total number of jobs.</li> <li><code>&lt;NUM-JOBS-DISPLAY&gt;</code>: Number of jobs to list in the table.</li> <li><code>&lt;TABLE&gt;</code>: Table of job data.</li> <li><code>&lt;UNUSED-HOURS&gt;</code>: Total number of unused GPU-hours.</li> </ul>"},{"location":"alert/time_limits_gpu/#usage","title":"Usage","text":"<p>Generate report for system administrators:</p> <pre><code>$ job_defense_shield --excessive-time-gpu\n</code></pre> <p>Send emails to offending users:</p> <pre><code>$ job_defense_shield --excessive-time-gpu --email\n</code></pre> <p>See which users have received emails and when:</p> <pre><code>$ job_defense_shield --excessive-time-gpu --check\n</code></pre>"},{"location":"alert/time_limits_gpu/#cron","title":"Cron","text":"<p>Below is an example <code>crontab</code> entry:</p> <pre><code>0 9 * * 1-5 /path/to/job_defense_shield --excessive-time-gpu --email -M della -r gpu,llm &gt; /path/to/log/excessive_time_gpu.log 2&gt;&amp;1\n</code></pre>"},{"location":"alert/too_many_cores_per_gpu/","title":"Too Many Allocated CPU-Cores per GPU","text":"<p>This alert identifies jobs that are allocating too many CPU-cores per GPU. The goal is to prevent the situation where there are free GPUs on a node but not enough CPU-cores to accept new jobs.</p>"},{"location":"alert/too_many_cores_per_gpu/#configuration-file","title":"Configuration File","text":"<p>Below is an example entry for <code>config.yaml</code>:</p> <pre><code>too-many-cores-per-gpu-1:\n  cluster: della\n  partitions:\n    - gpu\n  cores_per_node: 96\n  gpus_per_node: 8\n  cores_per_gpu_target: 12\n  cores_per_gpu_limit: 18\n  email_file: \"too_many_cores_per_gpu.txt\"\n  admin_emails:\n    - admin@institution.edu\n</code></pre> <p>The available settings are listed below:</p> <ul> <li> <p><code>cluster</code>: Specify the cluster name as it appears in the Slurm database.</p> </li> <li> <p><code>partitions</code>: Specify one or more Slurm partitions. Use <code>\"*\"</code> to include all partitions (i.e., <code>partitions: [\"*\"]</code>).</p> </li> <li> <p><code>gpus_per_node</code>: Number of GPUs per node.</p> </li> <li> <p><code>cores_per_gpu_target</code>: This should be the number of CPU-cores divided by the number of GPUs per node. For instance, for nodes with 96 cores and 8 GPUs, one should use 96/8=12.</p> </li> <li> <p><code>cores_per_gpu_limit</code>: Include jobs where the number of CPU-cores per GPU is equal to or greater than this value. One may set this equal to <code>cores_per_gpu_target</code> or a value slightly larger.</p> </li> <li> <p><code>cores_per_node</code>: Number of CPU-cores per node.</p> </li> <li> <p><code>email_file</code>: The text file to be used for the email message to users.</p> </li> </ul> <p><code>gpu_hours_threshold</code>: (Optional) Minimum number of GPU-hours (summed over the jobs) for the user to be considered. This setting makes it possible to ignore users that are not consuming many resources. Default: 0</p> <ul> <li> <p><code>cpu_eff_threshold</code>: (Optional) Ignore jobs with a CPU efficiency greater than this value. Default: 100</p> </li> <li> <p><code>min_run_time</code>: (Optional) Minimum run time in minutes for a job to be included in the calculation. For example, if <code>min_run_time: 30</code> is used then jobs that ran for less than 30 minutes are ignored. Default: 0</p> </li> <li> <p><code>include_running_jobs</code>: (Optional) If <code>True</code> then jobs in a state of <code>RUNNING</code> will be included in the calculation. The Prometheus server must be queried for each running job, which can be an expensive operation. Default: False</p> </li> <li> <p><code>nodelist</code>: (Optional) Only apply this alert to jobs that ran on the specified nodes. See example.</p> </li> <li> <p><code>excluded_qos</code>: (Optional) List of QOSes to exclude from this alert.</p> </li> <li> <p><code>excluded_partitions</code>: (Optional) List of partitions to exclude from this alert. This is useful when <code>partitions: [\"*\"]</code> is used.</p> </li> <li> <p><code>excluded_users</code>: (Optional) List of users to exclude from receiving emails.</p> </li> <li> <p><code>admin_emails</code>: (Optional) List of administrator email addresses that should receive copies of the emails that are sent to users.</p> </li> <li> <p><code>email_subject</code>: (Optional) Subject of the email message to users.</p> </li> <li> <p><code>report_title</code>: (Optional) Title of the report to system administrators.</p> </li> </ul>"},{"location":"alert/too_many_cores_per_gpu/#report-for-system-administrators","title":"Report for System Administrators","text":"<p>Below is an example report:</p> <pre><code>$ job_defense_shield --too-many-cores-per-gpu\n\n                                Too Many CPU-Cores Per GPU                                   \n-------------------------------------------------------------------------------\n JobID     User  Hours CPU-Eff  GPUs Cores-per-GPU  Cores-per-GPU-Target Emails\n-------------------------------------------------------------------------------\n62675166  u79355  1.8     5%      1         48                12          2 (1)\n62733079  u73812  1.3    15%      2         32                12          0   \n62735106  u73812  1.4    15%      2         32                12          0   \n62950436  u23992  1.2     7%      1         32                12          0   \n62952770  u23992  1.2     1%      1         32                12          0   \n-------------------------------------------------------------------------------\n   Cluster: della\nPartitions: gpu\n     Start: Tue Mar 04, 2025 at 11:32 AM\n       End: Tue Mar 18, 2025 at 11:32 AM\n</code></pre>"},{"location":"alert/too_many_cores_per_gpu/#email-message-to-users","title":"Email Message to Users","text":"<p>Below is an example email (see <code>email/too_many_cores_per_gpu.txt</code>):</p> <pre><code>Hello Alan (u12345),\n\nYour Della (PLI) jobs may be using more CPU-cores per GPU than necessary:\n\n    JobID   Hours CPU-Eff  Cores  GPUs Cores-per-GPU  Cores-per-GPU-Target\n   62733079  1.3    15%     64     2         32                12         \n   62735106  1.4    15%     64     2         32                12         \n\nEach node on Della (PLI) has 96 CPU-cores and 8 GPUs. If possible please try\nto use only up to 12 CPU-cores per GPU. This will prevent the situation\nwhere there are free GPUs on a node but not enough CPU-cores to accept new\njobs. CPU-Eff is the CPU efficiency.\n\nReplying to this automated email will open a support ticket with Research\nComputing.\n</code></pre>"},{"location":"alert/too_many_cores_per_gpu/#placeholders","title":"Placeholders","text":"<p>The following placeholders can be used in the email file:</p> <ul> <li><code>&lt;GREETING&gt;</code>: The greeting generated by <code>greeting-method</code>.</li> <li><code>&lt;CLUSTER&gt;</code>: The cluster specified for the alert (i.e., <code>cluster</code>).</li> <li><code>&lt;PARTITIONS&gt;</code>: A comma-separated list of partitions used by the user.</li> <li><code>&lt;TARGET&gt;</code>: The soft limit for the number of CPU-cores per GPU (i.e., <code>cores_per_gpu_target</code>).</li> <li><code>&lt;CORES&gt;</code>: Cores per node (i.e., <code>cores_per_node</code>).</li> <li><code>&lt;GPUS&gt;</code>: Number of GPUs per node (i.e., <code>gpus_per_node</code>).</li> <li><code>&lt;DAYS&gt;</code>: Number of days in the time window (default is 7).</li> <li><code>&lt;NUM-JOBS&gt;</code>: Total number of jobs with at least one idle GPU.</li> <li><code>&lt;TABLE&gt;</code>: Table of job data.</li> <li><code>&lt;JOBSTATS&gt;</code>: The <code>jobstats</code> command for the first job of the user.</li> </ul>"},{"location":"alert/too_many_cores_per_gpu/#usage","title":"Usage","text":"<p>Generate a report of the jobs allocating too many CPU cores per GPU:</p> <pre><code>$ job_defense_shield --too-many-cores-per-gpu\n</code></pre> <p>Send emails to offending users:</p> <pre><code>$ job_defense_shield --too-many-cores-per-gpu --email\n</code></pre> <p>See which users have received emails and when:</p> <pre><code>$ job_defense_shield --too-many-cores-per-gpu --check\n</code></pre>"},{"location":"alert/too_many_cores_per_gpu/#cron","title":"cron","text":"<p>Below is an example entry for <code>crontab</code>:</p> <pre><code>0 9 * * * /path/to/job_defense_shield --too-many-cores-per-gpu --email &gt; /path/to/log/too_many_cores_per_gpu.log 2&gt;&amp;1\n</code></pre>"},{"location":"alert/your_first_alert/","title":"Your First Alert","text":"<p>Here we demonstrate how to set up an alert that sends emails to users for excessive run time limits (e.g., requesting 3 days but only using 3 hours).</p>"},{"location":"alert/your_first_alert/#step-1-add-the-alert-to-the-configuration-file","title":"Step 1: Add the Alert to the Configuration File","text":"<p>Add the following to your <code>config.yaml</code> (below the \"Global Settings\") with the appropriate changes to <code>cluster</code> and <code>partitions</code>:</p> <pre><code>###############################\n## EXCESSIVE RUN TIME LIMITS ##\n###############################\nexcessive-time-cpu-1:\n  cluster: della\n  partitions:\n    - cpu\n  absolute_thres_hours:      0  # unused cpu-hours\n  overall_ratio_threshold: 1.0  # [0.0, 1.0]\n  num_top_users:            10  # count\n</code></pre> <p>Note</p> <p>One cluster per alert. Use multiple alerts for multiple clusters. Multiple partitions per alert is fine. See <code>example.yaml</code> in the GitHub repository to see this in practice.</p> <p>Multiple partitions can be entered as a YAML list:</p> <pre><code>  partitions:\n    - cpu\n    - bigmem\n    - serial\n</code></pre> <p>All partitions can be included with:</p> <pre><code>  partitions:\n    - \"*\"\n</code></pre> <p>Or equivalently:</p> <pre><code>  partitions: [\"*\"]\n</code></pre> <p>When all partitions are included, one can exclude specific partitions with <code>excluded_partitions</code>. An alternative to using <code>\"*\"</code> for a cluster with a large number of partitions is YAML anchors and aliases.</p> <p>Next, run the alert without <code>--email</code> so that no emails are sent but the output is displayed in the terminal:</p> <pre><code>$ job_defense_shield --excessive-time-cpu\n\n                         Excessive Time Limits                          \n------------------------------------------------------------------------\n  User   CPU-Hours  CPU-Hours  Ratio  Ratio   Ratio CPU-Rank Jobs Emails\n          (Unused)    (Used)  Overall  Mean  Median                     \n------------------------------------------------------------------------\n  u18587   495341      13716    0.03   0.03   0.02     10     643   0   \n  u81279   105666      80061    0.43   0.43   0.38      2      41   0   \n  u45521   105509      21595    0.17   0.17   0.13      8     109   0   \n  u73275    89469       8475    0.09   0.14   0.09     16      86   0   \n  u43409    88689     125583    0.59   0.59   0.59      1     279   0   \n  u39889    81659       1002    0.01   0.01   0.01     46    1718   0   \n  u42091    74348      52606    0.41   0.33   0.31      3    1437   0   \n  u75621    63365      21265    0.25   0.22   0.21      9    3741   0   \n  u60876    53043      31705    0.37   0.38   0.35      6    4425   0   \n  u41590    51491       9565    0.16   0.16   0.15     15     318   0   \n------------------------------------------------------------------------\n   Cluster: della\nPartitions: cpu\n     Start: Mon Mar 10, 2025 at 06:23 PM\n       End: Mon Mar 17, 2025 at 06:23 PM\n</code></pre> <p>CPU-Hours (Unused) is the product of the number of CPU-cores and the difference between the run time limit and the elapsed time (summed over all jobs). The table is sorted by this quantity. Ratio Overall is CPU-Hours (Used) divided by CPU-Hours (unused) plus CPU-Hours (used). CPU-Rank is the rank of the user by CPU-Hours (used). The user that has consumed the most CPU-hours has a rank of 1.</p> <p>Looking at the data in the table above, one can decide on the threshold values to use for the alert. The following choices look like good starting values:</p> <pre><code>excessive-time-cpu-1:\n  cluster: della\n  partitions:\n    - cpu\n  absolute_thres_hours: 100000  # unused cpu-hours\n  overall_ratio_threshold: 0.2  # [0.0, 1.0]\n  num_top_users:            10  # count\n</code></pre> <p>The settings above will only include users that have more than 100,000 unused (allocated) CPU-hours and a ratio of used to total of 0.2 or less.</p> <p>Let's run the alert again to check the filtering:</p> <pre><code>$ job_defense_shield --excessive-time-cpu\n\n                        Excessive Time Limits\n----------------------------------------------------------------------\n User  CPU-Hours  CPU-Hours  Ratio  Ratio   Ratio CPU-Rank Jobs Emails\n        (Unused)    (Used)  Overall  Mean  Median                     \n----------------------------------------------------------------------\nu18587   495341     13716     0.03   0.03   0.02     10    643    0   \nu45521   105509     21595     0.17   0.17   0.13      8    109    0   \n----------------------------------------------------------------------\n   Cluster: della\nPartitions: cpu\n     Start: Mon Mar 10, 2025 at 06:24 PM\n       End: Mon Mar 17, 2025 at 06:24 PM\n</code></pre> <p>This looks good. Only two users will receive an email.</p>"},{"location":"alert/your_first_alert/#step-2-prepare-the-email-file","title":"Step 2: Prepare the Email File","text":"<p>Next, look at your email file for this alert:</p> <pre><code>$ cat /path/to/email/excessive_time.txt\n</code></pre> <p>As you learned in the Emails section, you can modify this file just as you would any text file. Special placeholders can be used for each alert.</p> <p>Next, let's add the <code>email_file</code> to the alert entry:</p> <pre><code>excessive-time-cpu-1:\n  cluster: della\n  partitions:\n    - cpu\n  min_run_time:             61  # minutes\n  absolute_thres_hours: 100000  # unused cpu-hours\n  overall_ratio_threshold: 0.2  # [0.0, 1.0]\n  num_top_users:            10  # count\n  num_jobs_display:         10  # count\n  email_file: \"excessive_time.txt\"\n  admin_emails:\n    - admin@institution.edu\n</code></pre> <p>We also added a few of the optional settings which are covered in Excessive Run Time Limits.</p> <p>Make sure you set <code>email-files-path</code> in the global settings of <code>config.yaml</code> to the directory containing <code>excessive_time.txt</code></p>"},{"location":"alert/your_first_alert/#step-3-testing-the-emails","title":"Step 3: Testing the Emails","text":"<p>Let's run a test by adding the <code>--email</code> flag with the <code>--no-emails-to-users</code> modifier:</p> <pre><code>$ job_defense_shield --excessive-time-cpu --email --no-emails-to-users\n</code></pre> <p>The command above will only send emails to the addresses in <code>admin_emails</code>. Users will not receive emails. Make changes to your email message and then run the test again to see the new version.</p>"},{"location":"alert/your_first_alert/#step-4-send-the-emails-to-users","title":"Step 4: Send the Emails to Users","text":"<p>When you are satisfied with the email message, run the alert with only <code>--email</code> to send emails to the offending users:</p> <pre><code>$ job_defense_shield --excessive-time-cpu --email\n</code></pre> <p>Once again, those listed in <code>admin_emails</code> will receive copies of the emails.</p>"},{"location":"alert/your_first_alert/#step-5-examining-the-violation-files","title":"Step 5: Examining the Violation Files","text":"<p>Note that if you run the same command again it will not send any emails. This is because a user can only receive an email for a given alert once per week (by default).</p> <p>Take a look at the violation files that were written (with the path set by <code>violation-logs-path</code>):</p> <pre><code>$ ls /path/to/violations/excessive_time_limits_cpu/\nu18587.csv\nu45521.csv\n\n$ cat /path/to/violations/excessive_time_limits_cpu/u18587.csv\nUser,Cluster,Alert-Partitions,CPU-Hours-Unused,Jobs,Email-Sent\nu18587,della,cpu,497596,644,03/17/2025 18:35:58\n</code></pre> <p>The violation file of a user is read when determining whether or not sufficient time has passed to send another email. This is the purpose of the <code>Email-Sent</code> column. The software is written so that users receive at most one email about any given job.</p> <p>Changing partitions</p> <p>When deciding if a user should receive an email, the software first filters the violation file by <code>Cluster</code> and <code>Alert-Partitions</code>. <code>Alert-Partitions</code> is a comma-seperated string of the list of partitions. If you add or remove a partition to an alert this will change <code>Alert-Partitions</code> which may cause the user to receive a second email in less than seven days. This is not true when all paritions are included using <code>\"*\"</code>. In that case, partitions can be added and removed without concern for duplicate emails.</p>"},{"location":"alert/your_first_alert/#step-6-add-additional-alerts","title":"Step 6: Add Additional Alerts","text":"<p>Add as many alerts to the configuration file as you need to cover your partitions and clusters. Be sure to give them different names:</p> <pre><code>excessive-time-cpu-1:\n  cluster: della\n  ...\n\nexcessive-time-cpu-2:\n  cluster: stellar\n  ...\n</code></pre>"},{"location":"alert/your_first_alert/#step-7-update-crontab","title":"Step 7: Update <code>crontab</code>","text":"<p>Finally, add the appropriate entry to crontab. Something like:</p> <pre><code>0 9 * * 1-5 /path/to/job_defense_shield --excessive-time-cpu --email -M della -r cpu &gt; /path/to/log/excessive_time.log 2&gt;&amp;1\n</code></pre> <p>The <code>--excessive-time-cpu</code> flag will trigger all of the alerts of type <code>excessive-time-cpu</code>.</p> <p>Because our alert only needs data for the <code>cpu</code> partition of the <code>della</code> cluster, we used <code>-M della -r cpu</code>. This is not necessary but by default the data for all clusters and all partitions is requested from the Slurm database.</p> <p>To have a report sent to the addresses in <code>report-emails</code>, add the <code>--report</code> flag.</p>"},{"location":"alert/your_first_alert/#continue-by-adding-more-alerts","title":"Continue by Adding More Alerts","text":"<p>Take a look at the various alerts and add what you like to your configuration file. Here are three popular ones:</p> <ul> <li>Automatically Cancel GPU Jobs with 0% Utilization</li> <li>GPU-Hours at 0% Utilization</li> <li>Low GPU Efficiency</li> </ul>"},{"location":"alert/zero_cpu_util/","title":"Jobs with 0% CPU Utilization","text":"<p>This alert identifies jobs with 0% CPU utilization.</p> <p>The CPU utilization is calculated across all allocated CPU-cores on each node. A job will be included if the CPU utilization is 0% on any of the nodes. This alert is not capable of detecting individual CPU-cores that are idle unless the job only allocates one CPU-core per node.</p>"},{"location":"alert/zero_cpu_util/#configuration-file","title":"Configuration File","text":"<p>Below is an example entry for <code>config.yaml</code>:</p> <pre><code>zero-cpu-utilization-1:\n  cluster: stellar\n  partitions:\n    - cpu\n  min_run_time: 61 # minutes\n  email_file: \"zero_cpu_utilization.txt\"\n  admin_emails:\n    - admin@institution.edu\n</code></pre> <p>The parameters are explained below:</p> <ul> <li> <p><code>cluster</code>: Specify the cluster name as it appears in the Slurm database.</p> </li> <li> <p><code>partitions</code>: Specify one or more Slurm partitions. Use <code>\"*\"</code> to include all partitions (i.e., <code>partitions: [\"*\"]</code>).</p> </li> <li> <p><code>email_file</code>: The text file to be used for the email message to users.</p> </li> <li> <p><code>cpu_hours_threshold</code>: (Optional) Only users with greater than or equal to this number of CPU-hours at 0% utilization will receive an email. Default: 0</p> </li> <li> <p><code>min_run_time</code>: (Optional) Minimum run time of a job in units of minutes. If <code>min_run_time: 61</code> then jobs that ran for an hour or less are ignored. Default: 0</p> </li> <li> <p><code>include_running_jobs</code>: (Optional) If <code>True</code> then jobs in a state of <code>RUNNING</code> will be included in the calculation. The Prometheus server must be queried for each running job, which can be an expensive operation. Default: False</p> </li> <li> <p><code>nodelist</code>: (Optional) Only apply this alert to jobs that ran on the specified nodes. See example.</p> </li> <li> <p><code>excluded_qos</code>: (Optional) List of QOSes to exclude from this alert.</p> </li> <li> <p><code>excluded_partitions</code>: (Optional) List of partitions to exclude from this alert. This is useful when <code>partitions: [\"*\"]</code> is used.</p> </li> <li> <p><code>excluded_users</code>: (Optional) List of usernames to exclude from receiving emails.</p> </li> <li> <p><code>admin_emails</code>: (Optional) List of administrator email addresses that should receive copies of the emails that are sent to users.</p> </li> <li> <p><code>email_subject</code>: (Optional) Subject of the email message to users.</p> </li> <li> <p><code>report_title</code>: (Optional) Title of the report to system administrators.</p> </li> </ul>"},{"location":"alert/zero_cpu_util/#report-for-system-administrators","title":"Report for System Administrators","text":"<p>Below is an example report:</p> <pre><code>$ job_defense_shield --zero-cpu-utilization\n\n                     Jobs with Zero CPU Utilization                          \n---------------------------------------------------------------------------\n JobID    User   Nodes  Nodes-Unused  CPU-Util-Unused  Cores  Hours  Emails\n---------------------------------------------------------------------------\n1931133  u12345    11         11             0%        1056    48     3 (1)   \n1932935  u12345    11         11             0%        1056    48     0   \n1932937  u48726     8          4             0%         768     2     0   \n1933655  u52209     1          1             0%          96    24     0   \n---------------------------------------------------------------------------\n   Cluster: stellar\nPartitions: cpu, physics\n     Start: Wed Mar 12, 2025 at 02:50 PM\n       End: Wed Mar 19, 2025 at 02:50 PM\n</code></pre>"},{"location":"alert/zero_cpu_util/#email-message-to-users","title":"Email Message to Users","text":"<p>Below is an example email (see <code>email/zero_cpu_utilization.txt</code>):</p> <pre><code>Hello Alan (u12345),\n\nBelow are your recent jobs that did not use all of the allocated nodes:\n\n     JobID  Cluster  Nodes  Nodes-Unused CPU-Util-Unused  Cores Hours\n    1931133 stellar   11         11             0%        1056   48 \n    1932935 stellar   11         11             0%        1056   48 \n\nThe CPU utilization was found to be 0% on each of the unused nodes. Please\ninvestigate this issue before running additional jobs.\n\nReplying to this automated email will open a support ticket with Research\nComputing.\n</code></pre>"},{"location":"alert/zero_cpu_util/#placeholders","title":"Placeholders","text":"<p>The following placeholders can be used in the email file:</p> <ul> <li><code>&lt;GREETING&gt;</code>: The greeting generated by <code>greeting-method</code>.</li> <li><code>&lt;CLUSTER&gt;</code>: The cluster specified for the alert.</li> <li><code>&lt;PARTITIONS&gt;</code>: A comma-separated list of partitions used by the user.</li> <li><code>&lt;DAYS&gt;</code>: Number of days in the time window (default is 7).</li> <li><code>&lt;NUM-JOBS&gt;</code>: Number of jobs with 0% CPU utilization.</li> <li><code>&lt;TABLE&gt;</code>: Table of job data.</li> <li><code>&lt;JOBSTATS&gt;</code>: The <code>jobstats</code> command for the first job of the user.</li> </ul>"},{"location":"alert/zero_cpu_util/#usage","title":"Usage","text":"<p>Generate a report for system adminstrators:</p> <pre><code>$ job_defense_shield --zero-cpu-utilization\n</code></pre> <p>Send emails to offending users:</p> <pre><code>$ job_defense_shield --zero-cpu-utilization --email\n</code></pre> <p>See which users have received emails and when:</p> <pre><code>$ job_defense_shield --zero-cpu-utilization --check\n</code></pre>"},{"location":"alert/zero_cpu_util/#cron","title":"cron","text":"<p>Below is an example <code>crontab</code> entry:</p> <pre><code>0 9 * * 1-5 /path/to/job_defense_shield --zero-cpu-utilization --email -M della -r gpu,llm &gt; /path/to/log/zero_cpu_utilization.log 2&gt;&amp;1\n</code></pre>"},{"location":"alert/zero_gpu_util/","title":"GPU-Hours at 0% Utilization","text":"<p>This alert identifies users that have consumed the most GPU-hours at 0% utilization.</p> <p>To find users with low but non-zero GPU utilization then see low GPU efficiency. To automatically cancel GPU jobs at 0% utilization, see Cancel 0% GPU Jobs.</p>"},{"location":"alert/zero_gpu_util/#configuration-file","title":"Configuration File","text":"<p>Below is an example entry for <code>config.yaml</code>:</p> <pre><code>zero-util-gpu-hours-1:\n  cluster: della\n  partitions:\n    - gpu\n  min_run_time:              0  # minutes\n  gpu_hours_threshold_user: 24  # hours\n  gpu_hours_threshold_admin: 0  # hours\n  max_num_jobid:             4  # count\n  email_file: \"zero_util_gpu_hours.txt\"\n  admin_emails:\n    - admin@institution.edu\n</code></pre> <p>The available settings are listed below:</p> <ul> <li> <p><code>cluster</code>: Specify the cluster name as it appears in the Slurm database. One cluster name per alert.</p> </li> <li> <p><code>partitions</code>: Specify one or more Slurm partitions. Use <code>\"*\"</code> to include all partitions (i.e., <code>partitions: [\"*\"]</code>). The number of GPU-hours is summed over all partitions.</p> </li> <li> <p><code>min_run_time</code>: Minimum run time in minutes for a job to be included in the calculation. For example, if <code>min_run_time: 30</code> is used then jobs that ran for less than 30 minutes are ignored. Default: 0</p> </li> <li> <p><code>gpu_hours_threshold_user</code>: Only users with greater than or equal to this number of GPU-hours at 0% utilization will receive an email.</p> </li> <li> <p><code>gpu_hours_threshold_admin</code>: Only users with greater than or equal to this number of GPU-hours at 0% utilization will appear in the report for administrators.</p> </li> <li> <p><code>email_file</code>: The text file to be used as the email message to users.</p> </li> <li> <p><code>max_num_jobid</code>: (Optional) Maximum number of JobID's to show for a given user. If the number of jobs per user is greater than this value then a \"+\" character is appended to the end of the list. Default: 4</p> </li> <li> <p><code>include_running_jobs</code>: (Optional) If <code>True</code> then jobs in a state of <code>RUNNING</code> will be included in the calculation. The Prometheus server must be queried for each running job, which can be an expensive operation. Default: False</p> </li> <li> <p><code>nodelist</code>: (Optional) Only apply this alert to jobs that ran on the specified nodes. See example.</p> </li> <li> <p><code>excluded_qos</code>: (Optional) List of QOSes to exclude from this alert.</p> </li> <li> <p><code>excluded_partitions</code>: (Optional) List of partitions to exclude from this alert. This is useful when <code>partitions: [\"*\"]</code> is used.</p> </li> <li> <p><code>excluded_users</code>: (Optional) List of users to exclude from receiving emails.</p> </li> <li> <p><code>admin_emails</code>: (Optional) The emails sent to users will also be sent to these administator emails. This applies when the <code>--email</code> option is used.</p> </li> <li> <p><code>email_subject</code>: (Optional) Subject of the email message to users.</p> </li> <li> <p><code>report_title</code>: (Optional) Title of the report to system administrators.</p> </li> </ul> <p>For this alert, a GPU is said to have 0% utilization if all of the measurements made by the NVIDIA exporter over the entire job are zero. Measurements are typically made every 30 seconds or so.</p> <p>Multi-GPU Jobs</p> <p>For jobs that allocate multiple GPUs, only the GPU-hours for the GPUs at 0% utilization are included.</p> <p>Below is second example entry for <code>config.yaml</code>:</p> <pre><code>zero-util-gpu-hours:\n  cluster: stellar\n  partitions:\n    - gpu\n  min_run_time:              30  # minutes\n  gpu_hours_threshold_user:  24  # hours\n  gpu_hours_threshold_admin: 12  # hours\n  max_num_jobid:              3  # count\n  email_file: \"zero_util_gpu_hours.txt\"\n  admin_emails:\n    - admin@institution.edu\n</code></pre> <p>For the configuration above, only jobs that ran for 30 minutes or more are considered. Users will receive an email (when <code>--email</code> is used) if they consumed 24 GPU-hours or more at 0% utilization. System administrators will see users in the report (using <code>--report</code>) that consumed 12 GPU-hours or more. The JobID will be shown for up to three jobs per user.</p>"},{"location":"alert/zero_gpu_util/#report-for-system-administrators","title":"Report for System Administrators","text":"<p>Here is an example report:</p> <pre><code>$ job_defense_shield --zero-util-gpu-hours\n\n                           Zero Utilization GPU-Hours\n------------------------------------------------------------------------\n     User   GPU-Hours-At-0%  Jobs                     JobID                    \n------------------------------------------------------------------------\n1   u20461       397          16    60458831,60460188,60478799,60479839+\n2   u99704       196           8    60552976,60552983,60552984,60552985+\n3   u04204        62          39    60457297,60457395,60457408,60460181+\n4   u39983        32          40    60419086,60419088,60419089,60419090+\n5   u93550        22           6    60423037,60423668,60424743,60425344+\n6   u92847        17           5    60516409,60516469,60516554,60516718+\n7   u18225        17          17    60461780,60467419,60467445,60487739+\n8   u99455         9           4    60475110,60496234,60496390,60554903\n9   u30193         8           2                      60424873,60444734\n10  u62696         7          13    60422906,60540828,60545878,60545878+\n------------------------------------------------------------------------\n   Cluster: della\nPartitions: gpu, llm\n     Start: Thu Sep 1, 2024 at 08:00 AM\n       End: Thu Sep 8, 2024 at 08:00 AM\n</code></pre> <p>The table above shows that user <code>u20461</code> consumed 397 GPU-hours at 0% utilization. Four of the sixteen JobID's are shown.</p>"},{"location":"alert/zero_gpu_util/#email-message-to-users","title":"Email Message to Users","text":"<p>Below is an example email message:</p> <pre><code>Hello Alan (u12345),\n\nYou have consumed 120 GPU-hours at 0% GPU utilization in the past 7 days\non the pli-c partition(s) of della:\n\n     JobID     GPUs  GPUs-Unused GPU-Unused-Util  Zero-Util-GPU-Hours\n    62399648    8          8           0%                 36         \n    62399649    8          8           0%                 36          \n    62400616    8          8           0%                 36          \n    62402931    6          3           0%                 12          \n\nPlease address this issues before submitting new jobs. Replying to\nthis automated email will open a support ticket with Research\nComputing.\n</code></pre>"},{"location":"alert/zero_gpu_util/#placeholders","title":"Placeholders","text":"<p>The following placeholders can be used in the email file:</p> <ul> <li><code>&lt;GREETING&gt;</code>: The greeting generated by <code>greeting-method</code>.</li> <li><code>&lt;CLUSTER&gt;</code>: The cluster specified for the alert (i.e., <code>cluster</code>).</li> <li><code>&lt;PARTITIONS&gt;</code>: A comma-separated list of partitions used by the user.</li> <li><code>&lt;DAYS&gt;</code>: Number of days in the time window (default is 7).</li> <li><code>&lt;GPU-HOURS&gt;</code>: Total number of GPU-hours at 0% utilization.</li> <li><code>&lt;NUM-JOBS&gt;</code>: Total number of jobs with at least one idle GPU.</li> <li><code>&lt;TABLE&gt;</code>: Table of job data.</li> <li><code>&lt;JOBSTATS&gt;</code>: The <code>jobstats</code> command for the first job of the user.</li> </ul>"},{"location":"alert/zero_gpu_util/#usage","title":"Usage","text":"<p>Generate a report of the users with the most GPU-hours at 0% utilization:</p> <pre><code>$ job_defense_shield --zero-util-gpu-hours\n</code></pre> <p>Send emails to the offending users:</p> <pre><code>$ job_defense_shield --zero-util-gpu-hours --email\n</code></pre> <p>See which users have received emails and when they were sent:</p> <pre><code>$ job_defense_shield --zero-util-gpu-hours --check\n</code></pre>"},{"location":"alert/zero_gpu_util/#cron","title":"cron","text":"<p>Below is an example entry for <code>crontab</code>:</p> <pre><code>0 9 * * 1-5 /path/to/job_defense_shield --zero-util-gpu-hours --email &gt; /path/to/log/zero_util_gpu_hours.log 2&gt;&amp;1\n</code></pre>"},{"location":"reports/account/","title":"Usage by Slurm Account","text":"<p>This report shows the usage breakdown by cluster, partition, and Slurm account. Two tables are displayed.</p>"},{"location":"reports/account/#usage","title":"Usage","text":"<p>The command below will generate the report:</p> <pre><code>$ job_defense_shield --usage-by-slurm-account\n</code></pre> <p>One can produce the report for just one cluster:</p> <pre><code>$ job_defense_shield --usage-by-slurm-account -M stellar\n</code></pre>"},{"location":"reports/account/#configuration-file","title":"Configuration File","text":"<p>There are no settings for this report.</p>"},{"location":"reports/jobs_overview/","title":"Users with the Most Jobs","text":"<p>This report shows the users with the most jobs. Pending jobs are ignored.</p>"},{"location":"reports/jobs_overview/#usage","title":"Usage","text":"<pre><code>$ job_defense_shield --jobs-overview\n\n                  Users with the Most Jobs\n------------------------------------------------------------\n User  Cluster  Jobs  CPU   GPU   COM  CLD   F  RUN  OOM  TO\n------------------------------------------------------------\nu16463  della  49684 49682    2 49628   1   53   0     0   0\nu84550  della  12648 12648    0 12616  14    6  11     0   1\nu27577  della   9371  9371    0  9121  28  132  90     0   0\nu59120  della   6880  6880    0  4981 327  757 400    73 342\nu23097  della   5760  5760    0  3206  55    0   0  2494   5\nu46802  della   4919  3579 1340  4830  75   14   0     0   0\nu10876  della   4764  4572  192  4524  20   21   3     2  90\nu41607  della   3734  3734    0  3508 122   11   0    87   6\nu23720  della   3033  3033    0  2557  30  419   0    11  16\nu10567  della   2868  2868    0  2849  19    0   0     0   0\n------------------------------------------------------------\n     Start: Sun Mar 16, 2025 at 10:27 PM\n       End: Sun Mar 23, 2025 at 10:27 PM\n</code></pre>"},{"location":"reports/jobs_overview/#configuration-file","title":"Configuration File","text":"<p>There are no settings for this report.</p>"},{"location":"reports/longest_queued/","title":"Longest Queued Jobs","text":"<p>This report shows the jobs that have been queued for the longest. Note that some jobs may be dependencies. Some jobs may be pending due to a QOS limit. Array jobs are ignored. Only one job per user is shown.</p>"},{"location":"reports/longest_queued/#usage","title":"Usage","text":"<pre><code>$ job_defense_shield --longest-queued\n\n     Longest Queue Times (1 job per user, ignoring job arrays)     \n-------------------------------------------------------------------\n JobID    User  Cluster Nodes    QOS     Partition  Submit Eligible\n                                                    (Days)  (Days) \n-------------------------------------------------------------------\n62932214 u31448  della    1   gpu-medium        gpu   7       7    \n62948029 u35047  della    1   gpu-medium        gpu   6       6    \n62963093 u96241  della    2    gpu-short gpu-shared   6       6    \n62965916 u10786  della    1   gpu-medium        gpu   6       6    \n62964820 u25452  della    1    gpu-short gpu-shared   6       6    \n62979761 u88810  della    1   gpu-medium        gpu   5       5    \n62986554 u33030  della    4      pli-low        pli   5       5    \n63012946 u29346  della    1     gpu-long        gpu   4       4    \n62999688 u23026  della    1   gpu-medium        gpu   4       4    \n62998454 u87234  della    1    gpu-short gpu-shared   4       4    \n-------------------------------------------------------------------\n     Start: Sun Mar 16, 2025 at 09:13 PM\n       End: Sun Mar 23, 2025 at 09:13 PM\n</code></pre>"},{"location":"reports/longest_queued/#configuration-file","title":"Configuration File","text":"<p>There are no settings for this report.</p>"},{"location":"reports/most_cores/","title":"Jobs with the Most CPU-Cores","text":"<p>This report shows the jobs with the most allocated CPU-cores. Only one job per user is shown. Pending jobs are ignored.</p>"},{"location":"reports/most_cores/#usage","title":"Usage","text":"<pre><code>$ job_defense_shield --most-cores\n\n              Jobs with the Most CPU-Cores (1 Job per User)              \n--------------------------------------------------------------------------\n JobID    User   Cluster  Cores  Nodes  GPUs State Partition Hours CPU-Eff\n--------------------------------------------------------------------------\n1934156  u16627  stellar  4864    38     0    COM    cimes     21     94% \n1935647  u96942  stellar  4096    43     0      F      all    0.0      -- \n1936024  u56387  stellar  3840    40     0     TO     pppl     23     99% \n1935308  u49898  stellar  3840    40     0     TO     pppl     24     98% \n1934176  u24031  stellar  2304    18     0    COM    cimes      9     89% \n1938252  u42387  stellar  1920    20     0    COM       pu     24    100% \n1933647  u95519  stellar  1536    16     0    COM     pppl     22     99% \n1934482  u36180  stellar  1536    16     0     TO     pppl     24     99% \n 634428  u17874    tiger  1456    13     0    COM      cpu      3     91% \n 634182  u15289    tiger  1344    12     0     TO      cpu     48    100% \n--------------------------------------------------------------------------\n     Start: Sun Mar 16, 2025 at 07:55 PM\n       End: Sun Mar 23, 2025 at 07:55 PM\n</code></pre>"},{"location":"reports/most_cores/#configuration-file","title":"Configuration File","text":"<p>There are no settings for this report.</p>"},{"location":"reports/most_gpus/","title":"Jobs with the Most GPUs","text":"<p>This report shows the jobs with the most allocated GPUs. Only one job per user is shown. Pending jobs are ignored.</p>"},{"location":"reports/most_gpus/#usage","title":"Usage","text":"<pre><code>$ job_defense_shield --most-gpus\n\n                 Jobs with the Most GPUs (1 Job per User)                  \n--------------------------------------------------------------------------\n JobID     User  Cluster  GPUs Nodes  Cores  State Partition Hours GPU-Eff\n--------------------------------------------------------------------------\n63072881  u12345  della    64    8     392      F     pli-c   1.3     0%  \n63002445  u24197  della    32    4      64      F     pli-c   0.1    15%  \n62925866  u43975  della    16    2      64    CLD     pli-c    35    90%  \n62985649  u85040  della    16    2      32      F     pli-c   0.0     --  \n62998072  u51822  della    16    2      16      F     pli-c   0.2     0%  \n63057136  u65612  della    16    2      16      F       pli   0.2     8%  \n  634429  u77353  tiger    12    3      12    COM       gpu    10    71%  \n63017225  u79904  della     8    1      48    COM    pli-lc     3    13%  \n63027060  u32309  della     8    1       8      F     pli-c   0.0     --  \n63036518  u72188  della     8    1       1      F     pli-c   0.4     0%  \n--------------------------------------------------------------------------\n     Start: Sun Mar 16, 2025 at 08:19 PM\n       End: Sun Mar 23, 2025 at 08:19 PM\n</code></pre>"},{"location":"reports/most_gpus/#configuration-file","title":"Configuration File","text":"<p>There are no settings for this report.</p>"},{"location":"reports/overview/","title":"Reports for System Administrators","text":"<p>Any of the previously discussed GPU or CPU alerts can be turned into a report. This is done by simply adding the <code>--report</code> flag.</p> <pre><code>$ job_defense_shield --excess-cpu-memory --report\n</code></pre> <p>The <code>--report</code> flag will cause the report to be sent by email to the addresses in <code>report-emails</code> in <code>config.yaml</code>.</p> <p>One can also combine an email alert with the generation of a report:</p> <pre><code>$ job_defense_shield --excess-cpu-memory --email --report\n</code></pre> <p>The command above will email users for over-allocating CPU memory and send the report to system administrators.</p> <p>Below is a script for creating a comprehensive report. Such a report allows one to see all of the instances of underutilization and which users are scheduled to receive an email.</p> <pre><code>$ cat clusters_report.sh\n\n#!/bin/bash\nJDS=\"/home/admin/sw/jds-env\"\nCFG=\"${JDS}/config.yaml\"\n${JDS}/bin/job_defense_shield --report \\\n                              --config-file=${CFG} \\\n                              --zero-util-gpu-hours \\\n                              --low-gpu-efficiency \\\n                              --too-much-cpu-mem-per-gpu \\\n                              --too-many-cores-per-gpu \\\n                              --gpu-model-too-powerful \\\n                              --multinode-gpu-fragmentation \\\n                              --excessive-time-gpu \\\n                              --zero-cpu-utilization \\\n                              --excess-cpu-memory \\\n                              --low-cpu-efficiency \\\n                              --serial-allocating-multiple \\\n                              --multinode-cpu-fragmentation \\\n                              --excessive-time-cpu \\\n                              --usage-overview \\\n                              --longest-queued \\\n                              --most-gpus \\\n                              --most-cores \\\n                              --jobs-overview \\\n                              &gt; ${JDS}/log/report.log 2&gt;&amp;1\n</code></pre>"},{"location":"reports/usage_overview/","title":"Usage Overview","text":"<p>This report shows the usage by cluster as well as by cluster and partition. Two tables are displayed.</p>"},{"location":"reports/usage_overview/#usage","title":"Usage","text":"<pre><code>$ job_defense_shield --usage-overview\n\n        Usage Overview by Cluster\n-----------------------------------------\ncluster   users   cpu-hours    gpu-hours\n-----------------------------------------\n   della  464   1285938 (16%) 91714 (65%)\n stellar  149   6745324 (82%)  1926  (1%)\ntraverse    1    189987  (2%) 47497 (34%)\n-----------------------------------------\n\n\n\n       Usage Overview by Cluster and Partition\n------------------------------------------------------\ncluster  partition   users   cpu-hours     gpu-hours\n------------------------------------------------------\n   della        cpu  311    874114  (68%)     0   (0%)\n   della      pli-c   28    115406   (9%) 25838  (28%)\n   della gpu-shared   98     83617   (7%) 30083  (33%)\n   della    datasci   31     80954   (6%)     0   (0%)\n   della        gpu   51     47475   (4%) 16503  (18%)\n   della        pli   20     35814   (3%)  6249   (7%)\n   della     cryoem   17     20897   (2%)  4110   (4%)\n   della    physics    5     12968   (1%)     0   (0%)\n   della        mig   41      7169   (1%)  7169   (8%)\n   della     pli-lc    5      3107   (0%)  1081   (1%)\n   della    gputest   99      1948   (0%)   647   (1%)\n   della        all    1      1280   (0%)     0   (0%)\n   della      donia    4      1003   (0%)     0   (0%)\n   della     gpu-ee    2       173   (0%)    23   (0%)\n   della      grace    1        11   (0%)    11   (0%)\n   della      malik    1         2   (0%)     0   (0%)\n------------------------------------------------------\n stellar      cimes   21   2941001  (44%)     0   (0%)\n stellar         pu   56   2426873  (36%)     0   (0%)\n stellar       pppl   33   1340776  (20%)     0   (0%)\n stellar     serial   41     13187   (0%)     0   (0%)\n stellar        all   48     12377   (0%)     0   (0%)\n stellar        gpu   20     11044   (0%)  1926 (100%)\n stellar     bigmem    1        66   (0%)     0   (0%)\n------------------------------------------------------\ntraverse        all    1    189987 (100%) 47497 (100%)\n------------------------------------------------------\n     Start: Fri Mar 07, 2025 at 11:27 AM\n       End: Fri Mar 14, 2025 at 11:27 AM\n</code></pre>"},{"location":"reports/usage_overview/#configuration-file","title":"Configuration File","text":"<p>There are no settings for this report.</p>"}]}